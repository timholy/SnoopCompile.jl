var documenterSearchIndex = {"docs":
[{"location":"snoopi_deep_analysis/#Using-@snoopi_deep-results-to-improve-inferrability","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"As indicated in the workflow, the recommended steps to reduce latency are:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"check for invalidations\nadjust method specialization in your package or its dependencies\nfix problems in type inference\nadd precompile directives","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"The importance of fixing \"problems\" in type-inference was indicated in the tutorial: successful precompilation requires a chain of ownership, but runtime dispatch (when inference cannot predict the callee) results in breaks in this chain.  By improving inferrability, you can convert short, unconnected call-trees into a smaller number of large call-trees that all link back to your package(s).","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"In practice, it also turns out that opportunities to adjust specialization are often revealed by analyzing inference failures, so this page is complementary to the previous one.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Throughout this page, we'll use the OptimizeMe demo, which ships with SnoopCompile.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"note: Note\nTo understand what follows, it's essential to refer to OptimizeMe source code as you follow along.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> using SnoopCompile\n\njulia> cd(joinpath(pkgdir(SnoopCompile), \"examples\"))\n\njulia> include(\"OptimizeMe.jl\")\nMain.OptimizeMe\n\njulia> tinf = @snoopi_deep OptimizeMe.main()\nlotsa containers:\n7-element Vector{Main.OptimizeMe.Container}:\n Main.OptimizeMe.Container{Int64}(1)\n Main.OptimizeMe.Container{UInt8}(0x01)\n Main.OptimizeMe.Container{UInt16}(0xffff)\n Main.OptimizeMe.Container{Float32}(2.0f0)\n Main.OptimizeMe.Container{Char}('a')\n Main.OptimizeMe.Container{Vector{Int64}}([0])\n Main.OptimizeMe.Container{Tuple{String, Int64}}((\"key\", 42))\n3.14 is great\n2.718 is jealous\n6-element Vector{Main.OptimizeMe.Object}:\n Main.OptimizeMe.Object(1)\n Main.OptimizeMe.Object(2)\n Main.OptimizeMe.Object(3)\n Main.OptimizeMe.Object(4)\n Main.OptimizeMe.Object(5)\n Main.OptimizeMe.Object(7)\nInferenceTimingNode: 1.423913/2.713560 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 77 direct children\n\njulia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:2713559552))","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"If you visualize fg with ProfileView, you'll see something like this:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"(Image: flamegraph-OptimizeMe)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"From the standpoint of precompilation, this has some obvious problems:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"even though we called a single method, OptimizeMe.main(), there are many distinct flames separated by blank spaces. This indicates that many calls are being made by runtime dispatch:  each separate flame is a fresh entrance into inference.\nseveral of the flames are marked in red, indicating that they are not precompilable. While SnoopCompile does have the capability to automatically emit precompile directives for the non-red bars that sit on top of the red ones, in some cases the red extends to the highest part of the flame. In such cases there is no available precompile directive, and therefore no way to avoid the cost of type-inference.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Our goal will be to improve the design of OptimizeMe to make it more precompilable.","category":"page"},{"location":"snoopi_deep_analysis/#Analyzing-inference-triggers","page":"Using @snoopi_deep results to improve inferrability","title":"Analyzing inference triggers","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"We'll first extract the \"triggers\" of inference, which is just a repackaging of part of the information contained within tinf. Specifically an InferenceTrigger captures callee/caller relationships that straddle a fresh entrance to type-inference, allowing you to identify which calls were made by runtime dispatch and what MethodInstance they called.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> itrigs = inference_triggers(tinf)\n76-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for vect(::Int64, ::Vararg{Any, N} where N) from lotsa_containers (/home/tim/.julia/dev/SnoopCompile/examples/OptimizeMe.jl:13) with specialization MethodInstance for lotsa_containers()\n Inference triggered to call MethodInstance for promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N) from vect (./array.jl:126) with specialization MethodInstance for vect(::Int64, ::Vararg{Any, N} where N)\n Inference triggered to call MethodInstance for promote_typeof(::UInt8, ::UInt16, ::Vararg{Any, N} where N) from promote_typeof (./promotion.jl:272) with specialization MethodInstance for promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N)\n ⋮","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"This indicates that a whopping 76 calls were (1) made by runtime dispatch and (2) the callee had not previously been inferred. (There was a 77th call that had to be inferred, the original call to main(), but by default inference_triggers excludes calls made directly from top-level. You can change that through keyword arguments.)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"tip: Tip\nIn the REPL, SnoopCompile displays InferenceTriggers with yellow coloration for the callee, red for the caller method, and blue for the caller specialization. This makes it easier to quickly identify the most important information.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"In some cases, this might indicate that you'll need to fix 76 separate callers; fortunately, in many cases fixing the origin of inference problems can fix a number of later callees. To assist in interpreting these individual triggers, it's often helpful to organize them in a tree:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> itree = trigger_tree(itrigs)\nTriggerNode for root with 14 direct children\n\njulia> using AbstractTrees\n\njulia> print_tree(itree)\nroot\n├─ MethodInstance for vect(::Int64, ::Vararg{Any, N} where N)\n│  └─ MethodInstance for promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N)\n│     └─ MethodInstance for promote_typeof(::UInt8, ::UInt16, ::Vararg{Any, N} where N)\n│        └─ MethodInstance for promote_typeof(::UInt16, ::Float32, ::Vararg{Any, N} where N)\n│           └─ MethodInstance for promote_typeof(::Float32, ::Char, ::Vararg{Any, N} where N)\n│              ⋮\n│\n├─ MethodInstance for combine_eltypes(::Type, ::Tuple{Vector{Any}})\n│  ├─ MethodInstance for return_type(::Any, ::Any)\n│  ├─ MethodInstance for return_type(::Any, ::Any, ::UInt64)\n│  ├─ MethodInstance for return_type(::Core.Compiler.NativeInterpreter, ::Any, ::Any)\n│  ├─ MethodInstance for contains_is(::Core.SimpleVector, ::Any)\n│  └─ MethodInstance for promote_typejoin_union(::Type{Main.OptimizeMe.Container})\n├─ MethodInstance for Main.OptimizeMe.Container(::Int64)\n⋮","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"The parent-child relationships are based on the backtraces at the entrance to inference.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Let's start with the first of these.","category":"page"},{"location":"snoopi_deep_analysis/#suggest-and-a-fix-involving-manual-eltype-specification","page":"Using @snoopi_deep results to improve inferrability","title":"suggest and a fix involving manual eltype specification","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Because the analysis of inference failures is somewhat complex, SnoopCompile attempts to suggest an interpretation and/or remedy for each trigger:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> suggest(itree.children[1])\n/pathto/SnoopCompile/examples/OptimizeMe.jl:13: invoked callee is varargs (ignore this one, homogenize the arguments, declare an umbrella type, or force-specialize the callee MethodInstance for vect(::Int64, ::Vararg{Any, N} where N))\nimmediate caller(s):\n1-element Vector{Base.StackTraces.StackFrame}:\n main() at OptimizeMe.jl:42\n└─ ./array.jl:126: caller is varargs (ignore this one, specialize the caller vect(::Int64, ::Vararg{Any, N} where N) at array.jl:126, or improve inferrability of its caller)\n   immediate caller(s):\n   1-element Vector{Base.StackTraces.StackFrame}:\n    lotsa_containers() at OptimizeMe.jl:13\n   └─ ./promotion.jl:272: caller is varargs (ignore this one, specialize the caller promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N) at promotion.jl:272, or improve inferrability of its caller)\n      immediate caller(s):\n      1-element Vector{Base.StackTraces.StackFrame}:\n       vect(::Int64, ::Vararg{Any, N} where N) at array.jl:126\n      └─ ./promotion.jl:272: caller is varargs (ignore this one, specialize the caller promote_typeof(::UInt8, ::UInt16, ::Vararg{Any, N} where N) at promotion.jl:272, or improve inferrability of its caller)\n         immediate caller(s):\n         1-element Vector{Base.StackTraces.StackFrame}:\n          promote_typeof(::Int64, ::UInt8, ::Vararg{Any, N} where N) at promotion.jl:272\n         └─ ./promotion.jl:272: caller is varargs (ignore this one, specialize the caller promote_typeof(::UInt16, ::Float32, ::Vararg{Any, N} where N) at promotion.jl:272, or improve inferrability of its caller)\n            immediate caller(s):\n            1-element Vector{Base.StackTraces.StackFrame}:\n             promote_typeof(::UInt8, ::UInt16, ::Vararg{Any, N} where N) at promotion.jl:272\n            └─ ./promotion.jl:272: caller is varargs (ignore this one, specialize the caller promote_typeof(::Float32, ::Char, ::Vararg{Any, N} where N) at promotion.jl:272, or improve inferrability of its caller)\n               immediate caller(s):\n               1-element Vector{Base.StackTraces.StackFrame}:\n                promote_typeof(::UInt16, ::Float32, ::Vararg{Any, N} where N) at promotion.jl:272\n               ⋮","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"tip: Tip\nIn the REPL, interpretation are highlighted in color to help distinguish individual suggestions.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"In this case, the interpretation for the first node is \"invoked callee is varargs\" and suggestions are to choose one of \"ignore...homogenize...umbrella type...force-specialize\". Initially, this may seem pretty opaque. It helps if we look at the referenced line OptimizeMe.jl:13:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"list = [1, 0x01, 0xffff, 2.0f0, 'a', [0], (\"key\", 42)]","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"You'll notice above that the callee for the first node is vect; that's what handles the creation of the vector [1, ...]. If you look back up at the itree, you can see that a lot of promote_typeof calls follow, and you can see that the types listed in the arguments match the elements in list. The problem, here, is that vect has never been inferred for this particular combination of argument types, and the fact that the types are diverse means that Julia has decided not to specialize it for this combination. (If Julia had specialized it, it would have been inferred when lotsa_containers was inferred; the fact that it is showing up as a trigger means it wasn't.)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Let's see what kind of object this line creates:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> typeof(list)\nVector{Any} (alias for Array{Any, 1})","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Since it creates a Vector{Any}, perhaps we should just tell Julia to create such an object directly: we modify [1, 0x01, ...] to Any[1, 0x01, ...] (note the Any in front of [), so that Julia doesn't have to deduce the container type on its own. This follows the \"declare an umbrella type\" suggestion.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"note: Note\n\"Force-specialize\" means to encourage Julia to violate its heuristics and specialize the callee. Often this can be achieved by supplying a \"spurious\" type parameter. Examples include replacing higherorderfunction(f::Function, args...) with function higherorderfunction(f::F, args...) where F<:Function, or function getindex(A::MyArrayType{T,N}, idxs::Vararg{Int,N}) where {T,N} instead of just getindex(A::MyArrayType, idxs::Int...). (In the latter case, the N parameter is the crucial one: it forces specialization for a particular number of Int arguments.)This technique is not useful for the particular case we analyzed here, but it can be in other settings.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Making this simple 3-character fix eliminates that entire branch of the tree (a savings of 6 inference triggers).","category":"page"},{"location":"snoopi_deep_analysis/#eltypes-and-reducing-specialization-in-broadcast","page":"Using @snoopi_deep results to improve inferrability","title":"eltypes and reducing specialization in broadcast","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Let's move on to the next entry:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> print_tree(itree.children[2])\nMethodInstance for combine_eltypes(::Type, ::Tuple{Vector{Any}})\n├─ MethodInstance for return_type(::Any, ::Any)\n├─ MethodInstance for return_type(::Any, ::Any, ::UInt64)\n├─ MethodInstance for return_type(::Core.Compiler.NativeInterpreter, ::Any, ::Any)\n├─ MethodInstance for contains_is(::Core.SimpleVector, ::Any)\n└─ MethodInstance for promote_typejoin_union(::Type{Main.OptimizeMe.Container})\n\njulia> suggest(itree.children[2])\n./broadcast.jl:905: regular invoke (perhaps precompile lotsa_containers() at OptimizeMe.jl:14)\n├─ ./broadcast.jl:740: I've got nothing to say for MethodInstance for return_type(::Any, ::Any) consider `stacktrace(itrig)` or `ascend(itrig)`\n├─ ./broadcast.jl:740: I've got nothing to say for MethodInstance for return_type(::Any, ::Any, ::UInt64) consider `stacktrace(itrig)` or `ascend(itrig)`\n├─ ./broadcast.jl:740: I've got nothing to say for MethodInstance for return_type(::Core.Compiler.NativeInterpreter, ::Any, ::Any) consider `stacktrace(itrig)` or `ascend(itrig)`\n├─ ./broadcast.jl:740: I've got nothing to say for MethodInstance for contains_is(::Core.SimpleVector, ::Any) consider `stacktrace(itrig)` or `ascend(itrig)`\n└─ ./broadcast.jl:740: non-inferrable call, perhaps annotate combine_eltypes(f, args::Tuple) in Base.Broadcast at broadcast.jl:740 with type MethodInstance for promote_typejoin_union(::Type{Main.OptimizeMe.Container})\n   If a noninferrable argument is a type or function, Julia's specialization heuristics may be responsible.\n   immediate caller(s):\n   3-element Vector{Base.StackTraces.StackFrame}:\n    copy at broadcast.jl:905 [inlined]\n    materialize at broadcast.jl:883 [inlined]\n    lotsa_containers() at OptimizeMe.jl:14","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"While this tree is attributed to broadcast, you can see several references here to OptimizeMe.jl:14, which contains:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"cs = Container.(list)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Container.(list) is a broadcasting operation, and once again we find that this has inferrability problems. In this case, the initial suggestion \"perhaps precompile lotsa_containers\" is not helpful. (The \"regular invoke\" just means that the initial call was one where inference knew all the argument types, and hence in principle might be precompilable, but from this tree we see that this broke down in some of its callees.) Several children have no interpretation (\"I've got nothing to say...\"). Only the last one, \"non-inferrable call\", is (marginally) useful, it means that a call was made with arguments whose types could not be inferred.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"warning: Warning\nYou should always view these suggestions skeptically. Often, they flag downstream issues that are better addressed at the source; frequently the best fix may be at a line a bit before the one identified in a trigger, or even in a dependent callee of a line prior to the flagged one. This is a product of the fact that returning a non-inferrable argument is not the thing that forces a new round of inference; it's doing something (making a specialization-worthy call) with the object of non-inferrable type that triggers a fresh entrance into inference.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"How might we go about fixing this? One hint is to notice that itree.children[3] through itree.children[7] also ultimiately derive from this one line of OptimizeMe, but from a later line within broadcast.jl which explains why they are not bundled together with itree.children[2]. May of these correspond to creating different Container types, for example:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"└─ MethodInstance for restart_copyto_nonleaf!(::Vector{Main.OptimizeMe.Container}, ::Vector{Main.OptimizeMe.Container{Int64}}, ::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Tuple{Base.OneTo{Int64}}, Type{Main.OptimizeMe.Container}, Tuple{Base.Broadcast.Extruded{Vector{Any}, Tuple{Bool}, Tuple{Int64}}}}, ::Main.OptimizeMe.Container{UInt8}, ::Int64, ::Base.OneTo{Int64}, ::Int64, ::Int64)\n   ├─ MethodInstance for Main.OptimizeMe.Container(::UInt16)\n   ├─ MethodInstance for Main.OptimizeMe.Container(::Float32)\n   ├─ MethodInstance for Main.OptimizeMe.Container(::Char)\n   ├─ MethodInstance for Main.OptimizeMe.Container(::Vector{Int64})\n   └─ MethodInstance for Main.OptimizeMe.Container(::Tuple{String, Int64})","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"We've created a Container{T} for each specific T of the objects in list. In some cases, there may be good reasons for such specialization, and in such cases we just have to live with these inference failures. However, in other cases the specialization might be detrimental to compile-time and/or runtime performance. In such cases, we might decide to create them all as Container{Any}:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"cs = Container{Any}.(list)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"This 5-character change ends up eliminating 45 of our original 76 triggers. Not only did we eliminate the triggers from broadcasting, but we limited the number of different show(::IO, ::Container{T}) MethodInstances we need from later calls in main.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"When the Container constructor does more complex operations, in some cases you may find that Container{Any}(args...) still gets specialized for different types of args.... In such cases, you can create a special constructor that instructs Julia to avoid specialization in specific instances, e.g.,","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"struct Container{T}\n    field1::T\n    morefields...\n\n    # This constructor permits specialization on `args`\n    Container{T}(args...) where {T} = new{T}(args...)\n\n    # For Container{Any}, we prevent specialization\n    Container{Any}(@nospecialize(args...)) = new{Any}(args...)\nend","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"If you're following along, the best option is to make these fixes and go back to the beginning, re-collecting tinf and processing the triggers. We're down to 32 inference triggers.","category":"page"},{"location":"snoopi_deep_analysis/#typeasserts","page":"Using @snoopi_deep results to improve inferrability","title":"Adding type-assertions","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"If you've made the fixes above, the first child of itree is one for show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Container{Any}}); we'll skip that one for now, because it's a bit more sophisticated. Right below it, we see","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"├─ MethodInstance for combine_eltypes(::Type, ::Tuple{Vector{Any}})\n│  ├─ MethodInstance for return_type(::Any, ::Any)\n│  ├─ MethodInstance for return_type(::Any, ::Any, ::UInt64)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"and related nodes for similar, copyto_nonleaf!, etc., just as we saw above, so this looks like another case of broadcasting failure. In this case, suggest quickly indicates that it's the broadcasting in","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"function contain_list(list)\n    cs = Container.(list)\n    return concat_string(cs...)\nend","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Now we know the problem: main creates list = [2.718, \"is jealous\"], a vector with different object types, and this leads to inference failures in broadcasting. But wait, you might notice, contain_concrete gets called before contain_list, why doesn't it have a problem? The reason is that contain_concrete and its callee, concat_string, provide opportunities for inference to handle each object in a separate argument; the problems arise from bundling objects of different types into the same container.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"There are several ways we could go about fixig this example:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"we could delete contain_list altogether and use contain_concrete for everything.\nwe could try creating list as a tuple rather than a Vector{Any}; (small) tuples sometimes allow inference to succeed even when each element has a different type. This is as simple as changing list = [2.718, \"is jealous\"] to list = (2.718, \"is jealous\"), but whether it works to solve all your inference problems depends on the particular case.\nwe could use external knowledge to annotate the types of the items in list::Vector{Any}.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Here we'll illustrate the last of these, since it's the only one that's nontrivial. (It's also often a useful pattern in many real-world contexts, such as cases where you have a Dict{String,Any} but know something about the kinds of value-types associated with particular string keys.) We could rewrite contain_list so it looks like this:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"function contain_list(list)\n    length(list) == 2 || throw(DimensionMismatch(\"list must have length 2\"))\n    item1 = list[1]::Float64\n    item2 = list[2]::String\n    return contain_concrete(item1, item2)     # or we could repeat the body of contain_concrete\nend","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"The type-assertions tell inference that the corresponding items have the given types, and assist inference in cases where it has no mechanism to deduce the answer on its own. Julia will throw an error if the type-assertion fails. In some cases, a more forgiving option might be","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"item1 = convert(Float64, list[1])::Float64","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"which will attempt to convert list[1] to a Float64, and therefore handle a wider range of number types stored in the first element of list. Believe it or not, both the convert() and the ::Float64 type-assertion are necessary: since list[1] is of type Any, Julia will not be able to deduce which convert method will be used to perform the conversion, and it's always possible that someone has written a sloppy convert that doesn't return a value of the requested type. Without that final ::Float64, inference cannot simply assume that the result is a Float64. The type-assert ::Float64 enforces the fact that you're expecting that convert call to actually return a Float64–it will error if it fails to do so, and it's this error that allows inference to be certain that for the purposes of any later code it must be a Float64.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Of course, this just trades one form of inference failure for another–the call to convert will be made by runtime dispatch–but this can nevertheless be a big win for two reasons:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"even though the convert call will be made by runtime dispatch, in this particular case convert(Float64, ::Float64) is already compiled in Julia itself.  Consequently it doesn't require a fresh run of inference.\neven in cases where the types are such that convert might need to be inferred & compiled, the type-assertion allows Julia to assume that item1 is henceforth a Float64.  This makes it possible for inference to succeed for any code that follows.  When that's a large amount of code, the savings can be considerable.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Let's make that fix and also annotate the container type from main, list = Any[2.718, \"is jealous\"]. Just to see how we're progressing, we start a fresh session and discover we're down to 20 triggers with just three direct branches.","category":"page"},{"location":"snoopi_deep_analysis/#Vararg-homogenization","page":"Using @snoopi_deep results to improve inferrability","title":"Vararg homogenization","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"We'll again skip over the show branches (they are two of the remaining three), and focus on this one:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> node = itree.children[2]\nTriggerNode for MethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N) with 2 direct children\n\njulia> print_tree(node)\nMethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N)\n├─ MethodInstance for cat_similar(::UnitRange{Int64}, ::Type, ::Tuple{Int64})\n└─ MethodInstance for __cat(::Vector{Int64}, ::Tuple{Int64}, ::Tuple{Bool}, ::UnitRange{Int64}, ::Vararg{Any, N} where N)\n\njulia> suggest(node)\n./abstractarray.jl:1630: invoked callee is varargs (ignore this one, force-specialize the callee MethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N), or declare an umbrella type)\nimmediate caller(s):\n1-element Vector{Base.StackTraces.StackFrame}:\n main() at OptimizeMe.jl:48\n├─ ./abstractarray.jl:1636: caller is varargs (ignore this one, specialize the caller _cat_t(::Val{1}, ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N) at abstractarray.jl:1636, or improve inferrability of its caller)\n│  immediate caller(s):\n│  1-element Vector{Base.StackTraces.StackFrame}:\n│   cat_t(::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N; dims::Val{1}) at abstractarray.jl:1632\n└─ ./abstractarray.jl:1640: caller is varargs (ignore this one, specialize the caller _cat_t(::Val{1}, ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N) at abstractarray.jl:1640, or improve inferrability of its caller)\n   immediate caller(s):\n   1-element Vector{Base.StackTraces.StackFrame}:\n    cat_t(::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N; dims::Val{1}) at abstractarray.jl:1632","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Due to Julia's optimization and inlining, it's sometimes a bit hard to tell from these shortened displays where a particular trigger comes from. In this case we extract the specific trigger and show the stacktrace:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> itrig = node.itrig\nInference triggered to call MethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::UnitRange{Int64}, ::Vararg{Any, N} where N) from _cat (./abstractarray.jl:1630) inlined into MethodInstance for makeobjects() (/tmp/OptimizeMe.jl:39)\n\njulia> stacktrace(itrig)\n24-element Vector{Base.StackTraces.StackFrame}:\n exit_current_timer at typeinfer.jl:166 [inlined]\n typeinf(interp::Core.Compiler.NativeInterpreter, frame::Core.Compiler.InferenceState) at typeinfer.jl:208\n typeinf_ext(interp::Core.Compiler.NativeInterpreter, mi::Core.MethodInstance) at typeinfer.jl:835\n typeinf_ext_toplevel(interp::Core.Compiler.NativeInterpreter, linfo::Core.MethodInstance) at typeinfer.jl:868\n typeinf_ext_toplevel(mi::Core.MethodInstance, world::UInt64) at typeinfer.jl:864\n _cat at abstractarray.jl:1630 [inlined]\n #cat#127 at abstractarray.jl:1769 [inlined]\n cat at abstractarray.jl:1769 [inlined]\n vcat at abstractarray.jl:1698 [inlined]\n makeobjects() at OptimizeMe.jl:39\n main() at OptimizeMe.jl:48\n top-level scope at snoopi_deep.jl:53\n eval(m::Module, e::Any) at boot.jl:360\n eval_user_input(ast::Any, backend::REPL.REPLBackend) at REPL.jl:139\n repl_backend_loop(backend::REPL.REPLBackend) at REPL.jl:200\n start_repl_backend(backend::REPL.REPLBackend, consumer::Any) at REPL.jl:185\n run_repl(repl::REPL.AbstractREPL, consumer::Any; backend_on_current_task::Bool) at REPL.jl:317\n run_repl(repl::REPL.AbstractREPL, consumer::Any) at REPL.jl:305\n (::Base.var\"#872#874\"{Bool, Bool, Bool})(REPL::Module) at client.jl:387\n #invokelatest#2 at essentials.jl:707 [inlined]\n invokelatest at essentials.jl:706 [inlined]\n run_main_repl(interactive::Bool, quiet::Bool, banner::Bool, history_file::Bool, color_set::Bool) at client.jl:372\n exec_options(opts::Base.JLOptions) at client.jl:302\n _start() at client.jl:485","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"(You can also call stacktrace directly on node.) It's the lines immediately following typeinf_ext_toplevel that need concern us: you can see that the \"last stop\" on code we wrote here was makeobjects() at OptimizeMe.jl:39, after which it goes fairly deep into the concatenation pipeline before suffering an inference trigger at _cat at abstractarray.jl:1630.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"In this case, the first hint is quite useful, if we know how to interpret it. The invoked callee is varargs reassures us that the immediate caller, _cat, knows exactly which method it is calling (that's the meaning of the invoked). The real problem is that it doesn't know how to specialize it. The suggestion to homogenize the arguments is the crucial hint: the problem comes from the fact that in","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"xs = [1:5; 7]","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"1:5 is a UnitRange{Int} whereas 7 is an Int, and the fact that these are two different types prevents Julia from knowing how to specialize that varargs call. But this is easy to fix, because the result will be identical if we write this as","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"xs = [1:5; 7:7]","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"in which case both arguments are UnitRange{Int}, and this allows Julia to specialize the varargs call.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"note: Note\nIt's generally a good thing that Julia doesn't specialize each and every varargs call, because the lack of specialization reduces latency. However, when you can homogenize the argument types and make it inferrable, you make it more worthy of precompilation, which is a different and ultimately more impactful approach to latency reduction.","category":"page"},{"location":"snoopi_deep_analysis/#Defining-show-methods-for-custom-types","page":"Using @snoopi_deep results to improve inferrability","title":"Defining show methods for custom types","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Finally we are left with nodes that are related to show. We'll temporarily skip the first of these and examine","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> print_tree(node)\nMethodInstance for show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Object})\n└─ MethodInstance for var\"#sprint#386\"(::IOContext{Base.TTY}, ::Int64, ::typeof(sprint), ::Function, ::Main.OptimizeMe.Object)\n   └─ MethodInstance for sizeof(::Main.OptimizeMe.Object)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"We'll use this as an excuse to point out that if you don't know how to deal with the root node of this (sub)tree, you can tackle later nodes:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> itrigsnode = flatten(node)\n3-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Object}) from #38 (/home/tim/src/julia-master/usr/share/julia/stdlib/v1.6/REPL/src/REPL.jl:220) with specialization MethodInstance for (::REPL.var\"#38#39\"{REPL.REPLDisplay{REPL.LineEditREPL}, MIME{Symbol(\"text/plain\")}, Base.RefValue{Any}})(::Any)\n Inference triggered to call MethodInstance for var\"#sprint#386\"(::IOContext{Base.TTY}, ::Int64, ::typeof(sprint), ::Function, ::Main.OptimizeMe.Object) from sprint##kw (./strings/io.jl:101) inlined into MethodInstance for alignment(::IOContext{Base.TTY}, ::Vector{Main.OptimizeMe.Object}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::Int64, ::Int64, ::Int64) (./arrayshow.jl:68)\n Inference triggered to call MethodInstance for sizeof(::Main.OptimizeMe.Object) from _show_default (./show.jl:402) with specialization MethodInstance for _show_default(::IOContext{IOBuffer}, ::Any)\n\njulia> itrig = itrigsnode[end]\nInference triggered to call MethodInstance for sizeof(::Main.OptimizeMe.Object) from _show_default (./show.jl:402) with specialization MethodInstance for _show_default(::IOContext{IOBuffer}, ::Any)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"The stacktrace begins","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> stacktrace(itrig)\n35-element Vector{Base.StackTraces.StackFrame}:\n exit_current_timer at typeinfer.jl:166 [inlined]\n typeinf(interp::Core.Compiler.NativeInterpreter, frame::Core.Compiler.InferenceState) at typeinfer.jl:208\n typeinf_ext(interp::Core.Compiler.NativeInterpreter, mi::Core.MethodInstance) at typeinfer.jl:835\n typeinf_ext_toplevel(interp::Core.Compiler.NativeInterpreter, linfo::Core.MethodInstance) at typeinfer.jl:868\n typeinf_ext_toplevel(mi::Core.MethodInstance, world::UInt64) at typeinfer.jl:864\n _show_default(io::IOContext{IOBuffer}, x::Any) at show.jl:402\n show_default at show.jl:395 [inlined]\n show(io::IOContext{IOBuffer}, x::Any) at show.jl:390\n sprint(f::Function, args::Main.OptimizeMe.Object; context::IOContext{Base.TTY}, sizehint::Int64) at io.jl:103\n⋮","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"You can see that sprint called show which called _show_default; _show_default clearly needed to call sizeof. The hint, in this case, suggests the impossible:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> suggest(itrig)\n./show.jl:402: non-inferrable call, perhaps annotate _show_default(io::IO, x) in Base at show.jl:397 with type MethodInstance for sizeof(::Main.OptimizeMe.Object)\nIf a noninferrable argument is a type or function, Julia's specialization heuristics may be responsible.\nimmediate caller(s):\n2-element Vector{Base.StackTraces.StackFrame}:\n show_default at show.jl:395 [inlined]\n show(io::IOContext{IOBuffer}, x::Any) at show.jl:390","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Because Base doesn't know about OptimizeMe.Object, you could not add such an annotation, and it wouldn't be correct in the vast majority of cases.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"As the name implies, _show_default is the fallback show method. We can fix this by adding our own show method","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Base.show(io::IO, o::Object) = print(io, \"Object x: \", o.x)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"to the module definition. Object is so simple that this is slightly silly, but in more complex cases adding good show methods improves usability of your packages tremendously. (SnoopCompile has many show specializations, and without them it would be practically unusable.)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"When you do define a custom show method, you own it, so of course it will be precompilable. So we've circumvented this particular issue.","category":"page"},{"location":"snoopi_deep_analysis/#Creating-\"warmup\"-methods","page":"Using @snoopi_deep results to improve inferrability","title":"Creating \"warmup\" methods","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Finally, it is time to deal with those long-delayed show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::T) triggers and the triggers they inspire. We have two of them, one for T = Vector{Main.OptimizeMe.Container{Any}} and one for T = Vector{Main.OptimizeMe.Object}. Let's look at just the trigger associated with the first:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> itrig\nInference triggered to call MethodInstance for show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMeFixed.Container{Any}}) from #38 (/pathto/julia/usr/share/julia/stdlib/v1.6/REPL/src/REPL.jl:220) with specialization MethodInstance for (::REPL.var\"#38#39\"{REPL.REPLDisplay{REPL.LineEditREPL}, MIME{Symbol(\"text/plain\")}, Base.RefValue{Any}})(::Any)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"In this case we see that the method is #38.  This is a gensym, or generated symbol, indicating that the method was generated during Julia's lowering pass, and might indicate a macro, a do block or other anonymous function, the generator for a @generated function, etc.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"warning: Warning\nIt's particularly worth your while to improve inferrability for gensym-methods. The number assiged to a gensymmed-method may change as you or other developers modify the package (possibly due to changes at very difference source-code locations), and so any explicit precompile directives involving gensyms may not have a long useful life.But not all methods with # in their name are problematic: methods ending in ##kw or that look like ##funcname#39 are keyword and body methods, respectively, for methods that accept keywords.  They can be obtained from the main method, and so precompile directives for such methods will not be outdated by incidental changes to the package.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"edit(itrig) (or equivalently, edit(node) where node is a child of itree) takes us to this method in Base:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"function display(d::REPLDisplay, mime::MIME\"text/plain\", x)\n    x = Ref{Any}(x)\n    with_repl_linfo(d.repl) do io\n        io = IOContext(io, :limit => true, :module => Main::Module)\n        get(io, :color, false) && write(io, answer_color(d.repl))\n        if isdefined(d.repl, :options) && isdefined(d.repl.options, :iocontext)\n            # this can override the :limit property set initially\n            io = foldl(IOContext, d.repl.options.iocontext, init=io)\n        end\n        show(io, mime, x[])\n        println(io)\n    end\n    return nothing\nend","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"The generated method corresponds to the do block here. The call to show comes from show(io, mime, x[]). This implementation uses a clever trick, wrapping x in a Ref{Any}(x), to prevent specialization of the method defined by the do block on the specific type of x. This trick is designed to limit the number of MethodInstances inferred for this display method.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Unfortunately, from the standpoint of precompilation we have something of a conundrum. It turns out that this trigger corresponds to the first of the big red flames in the flame graph. show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMe.Container{Any}}) is not precompilable because Base owns the show method for Vector; we might own the element type, but we're leveraging the generic machinery in Base and consequently it owns the method. If these were all packages, you might request its developers to add a precompile directive, but that will work only if the package that owns the method knows about the relevant type. In this situation, Julia's Base module doesn't know about OptimizeMe.Container{Any}, so we're stuck.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"There are a couple of ways one might go about improving matters. First, one option is that this should be changed in Julia itself: since the caller, display, has gone to some lengths to reduce specialization, it would be worth contemplating whether show(io::IO, ::MIME\"text/plain\", X::AbstractArray) should have a @nospecialize around X. Here, we'll pursue a simple \"cheat,\" one that allows us to directly precompile this method. The trick is to link it, via a chain of backedges, to a method that our package owns:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"# \"Stub\" callers for precompilability (we don't use this function for any real work)\nfunction warmup()\n    mime = MIME(\"text/plain\")\n    io = Base.stdout::Base.TTY\n    # Container{Any}\n    v = [Container{Any}(0)]\n    show(io, mime, v)\n    show(IOContext(io), mime, v)\n    # Object\n    v = [Object(0)]\n    show(io, mime, v)\n    show(IOContext(io), mime, v)\n    return nothing\nend\n\nprecompile(warmup, ())","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"We handled not just Vector{Container{Any}} but also Vector{Object}, since that turns out to correspond to the other wide block of red bars. If you make this change, start a fresh session, and recreate the flame graph, you'll see that the wide red flames are gone:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"(Image: flamegraph-OptimizeMeFixed)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"info: Info\nIt's worth noting that this warmup method needed to be carefully written to succeed in its mission. stdout is not inferrable (it's a global that can be replaced by redirect_stdout), so we needed to annotate its type. We also might have been tempted to use a loop, for io in (stdout, IOContext(stdout)) ... end, but inference needs a dedicated call-site where it knows all the types. (Union-splitting can sometimes come to the rescue, but not if the list is long or elements non-inferrable.) The safest option is to make each call from a separate site in the code.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"The next trigger, a call to sprint from inside Base.alignment(io::IO, x::Any), could also be handled using this warmup trick, but the flamegraph says this call (also marked in red) isn't an expensive method to infer.  In such cases, it's fine to choose to leave it be.","category":"page"},{"location":"snoopi_deep_analysis/#Implementing-or-requesting-precompile-directives-in-upstream-packages","page":"Using @snoopi_deep results to improve inferrability","title":"Implementing or requesting precompile directives in upstream packages","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Of the remaining triggers (now numbering 14), the flamegraph indicates that the most expensive inference run is","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Inference triggered to call MethodInstance for show(::IOContext{IOBuffer}, ::Float32) from _show_default (./show.jl:412) with specialization MethodInstance for _show_default(::IOContext{IOBuffer}, ::Any)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"You can check that by listing the children of ROOT in order of inclusive time:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> nodes = sort(tinf.children; by=inclusive)\n14-element Vector{SnoopCompileCore.InferenceTimingNode}:\n InferenceTimingNode: 0.000053/0.000053 on InferenceFrameInfo for ==(::Type, nothing::Nothing) with 0 direct children\n InferenceTimingNode: 0.000054/0.000054 on InferenceFrameInfo for sizeof(::Main.OptimizeMeFixed.Container{Any}) with 0 direct children\n InferenceTimingNode: 0.000061/0.000061 on InferenceFrameInfo for Base.typeinfo_eltype(::Type) with 0 direct children\n InferenceTimingNode: 0.000075/0.000380 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Any) with 1 direct children\n InferenceTimingNode: 0.000445/0.000445 on InferenceFrameInfo for Pair{Symbol, DataType}(::Any, ::Any) with 0 direct children\n InferenceTimingNode: 0.000663/0.000663 on InferenceFrameInfo for print(::IOContext{Base.TTY}, ::String, ::String, ::Vararg{String, N} where N) with 0 direct children\n InferenceTimingNode: 0.000560/0.001049 on InferenceFrameInfo for Base.var\"#sprint#386\"(::IOContext{Base.TTY}, ::Int64, sprint::typeof(sprint), ::Function, ::Main.OptimizeMeFixed.Object) with 4 direct children\n InferenceTimingNode: 0.000441/0.001051 on InferenceFrameInfo for Pair(::Symbol, ::Type) with 1 direct children\n InferenceTimingNode: 0.000627/0.001140 on InferenceFrameInfo for Base.var\"#sprint#386\"(::IOContext{Base.TTY}, ::Int64, sprint::typeof(sprint), ::Function, ::Main.OptimizeMeFixed.Container{Any}) with 4 direct children\n InferenceTimingNode: 0.000321/0.001598 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::UInt16) with 4 direct children\n InferenceTimingNode: 0.000190/0.012516 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Vector{Int64}) with 3 direct children\n InferenceTimingNode: 0.021179/0.033940 on InferenceFrameInfo for Base.Ryu.writeshortest(::Vector{UInt8}, ::Int64, ::Float32, ::Bool, ::Bool, ::Bool, ::Int64, ::UInt8, ::Bool, ::UInt8, ::Bool, ::Bool) with 29 direct children\n InferenceTimingNode: 0.000083/0.035496 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Tuple{String, Int64}) with 1 direct children\n InferenceTimingNode: 0.000188/0.092555 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Float32) with 1 direct children","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"You can see it's the most expensive remaining root, weighing in at nearly 100ms. This method is defined in the Base.Ryu module,","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> node = nodes[end]\nInferenceTimingNode: 0.000188/0.092555 on InferenceFrameInfo for show(::IOContext{IOBuffer}, ::Float32) with 1 direct children\n\njulia> Method(node)\nshow(io::IO, x::T) where T<:Union{Float16, Float32, Float64} in Base.Ryu at ryu/Ryu.jl:111","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Now, we could add this to warmup and at least solve the inference problem. However, on the flamegraph you might note that this is followed shortly by a couple of calls to Ryu.writeshortest (the third-most expensive to infer), followed by a long gap. That hints that other steps, like native code generation, may be expensive. Since these are base Julia methods, and Float32 is a common type, it would make sense to file an issue or pull request that Julia should come shipped with these precompiled–that would cache not only the type-inference but also the native code, and thus represents a far more complete solution.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Later, we'll see how parcel can generate such precompile directives automatically, so this is not a step you need to implement entirely on your own.","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"Another show MethodInstance, show(::IOContext{IOBuffer}, ::Tuple{String, Int64}), seems too specific to be worth worrying about, so we call it quits here.","category":"page"},{"location":"snoopi_deep_analysis/#Advanced-analysis:-ascend","page":"Using @snoopi_deep results to improve inferrability","title":"Advanced analysis: ascend","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"One thing that hasn't yet been covered is that when you really need more insight, you can use ascend:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> itrig = itrigs[5]\nInference triggered to call MethodInstance for show(::IOContext{IOBuffer}, ::Float32) from _show_default (./show.jl:412) with specialization MethodInstance for _show_default(::IOContext{IOBuffer}, ::Any)\n\njulia> ascend(itrig)\nChoose a call for analysis (q to quit):\n >   show(::IOContext{IOBuffer}, ::Float32)\n       _show_default(::IOContext{IOBuffer}, ::Any) at ./show.jl:412\n         show_default at ./show.jl:395 => show(::IOContext{IOBuffer}, ::Any) at ./show.jl:390\n           #sprint#386(::IOContext{Base.TTY}, ::Int64, ::typeof(sprint), ::Function, ::Main.OptimizeMeFixed.Container{Any}) at ./strings/io.jl:103\n             sprint##kw at ./strings/io.jl:101 => alignment at ./show.jl:2528 => alignment(::IOContext{Base.TTY}, ::Vector{Main.OptimizeMeFixed.Container{Any}}, ::UnitRange{Int64}, ::UnitRange{Int64}, ::\n               print_matrix(::IOContext{Base.TTY}, ::AbstractVecOrMat{T} where T, ::String, ::String, ::String, ::String, ::String, ::String, ::Int64, ::Int64) at ./arrayshow.jl:197\n                 print_matrix at ./arrayshow.jl:169 => print_array at ./arrayshow.jl:323 => show(::IOContext{Base.TTY}, ::MIME{Symbol(\"text/plain\")}, ::Vector{Main.OptimizeMeFixed.Container{Any}}) at ./a\n                   (::REPL.var\"#38#39\"{REPL.REPLDisplay{REPL.LineEditREPL}, MIME{Symbol(\"text/plain\")}, Base.RefValue{Any}})(::Any) at /home/tim/src/julia-master/usr/share/julia/stdlib/v1.6/REPL/src/REPL\n                     with_repl_linfo(::Any, ::REPL.LineEditREPL) at /home/tim/src/julia-master/usr/share/julia/stdlib/v1.6/REPL/src/REPL.jl:462\nv                      display(::REPL.REPLDisplay, ::MIME{Symbol(\"text/plain\")}, ::Any) at /home/tim/src/julia-master/usr/share/julia/stdlib/v1.6/REPL/src/REPL.jl:213\n","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"ascend was covered in much greater detail in fixing invalidations, and you can read about using it on that page. Here, one twist is that some lines contain content like","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"show_default at ./show.jl:395 => show(::IOContext{IOBuffer}, ::Any) at ./show.jl:390","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"This indicates that show_default was inlined into show. ascend needs the full non-inlined MethodInstance to descend into, so the tree only includes such nodes. However, within Cthulhu you can toggle optimization and thereby descend into some of these inlined method, or see the full consequence of their inlining into the caller.","category":"page"},{"location":"snoopi_deep_analysis/#A-note-on-analyzing-test-suites","page":"Using @snoopi_deep results to improve inferrability","title":"A note on analyzing test suites","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"If you're doing a package analysis, it's convenient to use the package's runtests.jl script as a way to cover much of the package's functionality. SnoopCompile has a couple of enhancements designed to make it easier to ignore inference triggers that come from the test suite itself. First, suggest.(itrigs) may show something like this:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":" ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)\n ./broadcast.jl:1315: inlineable (ignore this one)","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"This indicates a broadcasting operation in the @testset itself. Second, while it's a little dangerous (because suggest cannot entirely be trusted), you can filter these out:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> itrigsel = [itrig for itrig in itrigs if !isignorable(suggest(itrig))];\n\njulia> length(itrigs)\n222\n\njulia> length(itrigsel)\n71","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"While there is some risk of discarding triggers that provide clues about the origin of other triggers (e.g., they would have shown up in the same branch of the trigger_tree), the shorter list may help direct your attention to the \"real\" issues.","category":"page"},{"location":"snoopi_deep_analysis/#Results-from-the-improvements","page":"Using @snoopi_deep results to improve inferrability","title":"Results from the improvements","text":"","category":"section"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"An improved version of OptimizeMe can be found in OptimizeMeFixed.jl in the same directory. Let's see where we stand:","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"julia> tinf = @snoopi_deep OptimizeMeFixed.main()\n3.14 is great\n2.718 is jealous\n...\n Object x: 7\nInferenceTimingNode: 0.888522055/1.496965222 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 15 direct children","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"We've substantially shrunk the overall inclusive time from 2.68s to about 1.5s. Some of this came from our single precompile directive, for warmup. But even more of it came from limiting specialization (using Container{Any} instead of Container) and by making some results easier on type-inference (e.g., our changes for the vcat pipeline).","category":"page"},{"location":"snoopi_deep_analysis/","page":"Using @snoopi_deep results to improve inferrability","title":"Using @snoopi_deep results to improve inferrability","text":"On the next page, we'll wrap all this up with more explicit precompile directives.","category":"page"},{"location":"snoopr/#invalidations","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"compat: Compat\n@snoopr is available on Julia 1.6.0-DEV.154 or above, but the results can be relevant for all Julia versions.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Invalidations occur when there is a danger that new methods would supersede older methods in previously-compiled code. For safety, Julia's compiler invalidates that old code, marking it as unsuitable for use; the next time you call that method, it will have to be compiled again from scratch. (If no one ever needs that method again, there is no major loss.)","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Some packages define new methods that force invalidation of previously-compiled code. If your package, or any of your dependencies, triggers many invalidations, it has several bad effects:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"any invalidated methods you need for the functionality in your package will have to be recompiled. This will lead to a direct (and occasionally large) slowdown for your package.\ninvalidations by your dependencies (packages you rely on) can block precompilation of methods in your package, preventing you from taking advantage of some the other features of SnoopCompile.\neven if you don't need the invalidated code for your package, any invalidations triggered by your package might harm packages that depend on yours.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"For these reasons, it's advisable to begin by analyzing invalidations. On recent Julia versions, most packages do not trigger a large number of invalidations; often, all that is needed is a quick glance at invalidations before moving on to the next step. Occasionally, checking for invalidations can save you a lot of confusion and frustration at later steps, so it is well worth taking a look.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Readers who want more background and context are encouraged to read this blog post.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"note: Note\nInvalidatons occur only for compiled code; method definitions themselves cannot be invalidated. As a consequence, it's possible to have latent invalidation risk; this risk can become exposed if you use some intermediate functionality before loading your package, or if your dependencies someday add precompile directives. So even if you've checked for invalidations previously, sometimes it's worth taking a fresh look.","category":"page"},{"location":"snoopr/#Recording-invalidations","page":"Snooping on and fixing invalidations: @snoopr","title":"Recording invalidations","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"DocTestFilters = r\"(REPL\\[\\d+\\]|none):\\d+\"\nDocTestSetup = quote\n    using SnoopCompile\nend","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"To record the invalidations caused by defining new methods, use @snoopr. @snoopr is exported by SnoopCompile, but the recommended approach is to record invalidations using the minimalistic SnoopCompileCore package, and then load SnoopCompile to do the analysis:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"using SnoopCompileCore\ninvalidations = @snoopr begin\n    # package loads and/or method definitions that might invalidate other code\nend\nusing SnoopCompile   # now that we've collected the data, load the complete package to analyze the results","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"note: Note\nSnoopCompileCore was split out from SnoopCompile to reduce the risk of invalidations from loading SnoopCompile itself. Once a MethodInstance gets invalidated, it doesn't show up in future @snoopr results, so anything that gets invalidated in order to provide @snoopr would be omitted from the results. SnoopCompileCore is a very small package with no dependencies and which avoids extending any of Julia's own functions, so it cannot invalidate any other code.","category":"page"},{"location":"snoopr/#Analyzing-invalidations","page":"Snooping on and fixing invalidations: @snoopr","title":"Analyzing invalidations","text":"","category":"section"},{"location":"snoopr/#A-first-example","page":"Snooping on and fixing invalidations: @snoopr","title":"A first example","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"We'll walk through this process with the following example:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> f(::Real) = 1;\n\njulia> callf(container) = f(container[1]);\n\njulia> call2f(container) = callf(container);","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Because code doesn't get compiled until it gets run, and invalidations only affect compiled code, let's run this with three different container types:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> c64  = [1.0]; c32 = [1.0f0]; cabs = AbstractFloat[1.0];  # Vector{Float64}, Vector{Float32}, and Vector{AbstractFloat}, respectively\n\njulia> call2f(c64)\n1\n\njulia> call2f(c32)\n1\n\njulia> call2f(cabs)\n1","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"warning: Warning\nIf you're following along, be sure you actually execute these methods, or you won't obtain the results below.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Now we'll define a new f method, one specialized for Float64. So we can see the consequences for the compiled code, we'll make this definition while snooping on the compiler with @snoopr:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> using SnoopCompileCore\n\njulia> invalidations = @snoopr f(::Float64) = 2;","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"As should be apparent, running call2f on c64 should produce a different result than formerly, so Julia certainly needs to invalidate that code.  Let's see what that looks like. The simplest thing we can do is list or count invalidations:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> using SnoopCompile\n\njulia> length(uinvalidated(invalidations))  # collect the unique MethodInstances & count them\n6","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"The length of this set is your simplest insight into the extent of invalidations triggered by this method definition.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"If you want to fix invalidations, it's crucial to know why certain MethodInstances were invalidated. For that, it's best to use a tree structure, in which children are invalidated because their parents get invalidated:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> trees = invalidation_trees(invalidations)\n1-element Vector{SnoopCompile.MethodInvalidations}:\n inserting f(::Float64) in Main at REPL[9]:1 invalidated:\n   backedges: 1: superseding f(::Real) in Main at REPL[2]:1 with MethodInstance for f(::Float64) (2 children)\n              2: superseding f(::Real) in Main at REPL[2]:1 with MethodInstance for f(::AbstractFloat) (2 children)\n   1 mt_cache","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"The output, trees, is a vector of MethodInvalidations, a data type defined in SnoopCompile; each of these is the set of invalidations triggered by a particular method definition. In this case, we only defined one method, so we can get at most one MethodInvalidation. @snoopr using SomePkg might result in a list of such objects, each connected to a particular method defined in a particular package (either SomePkg itself or one of its dependencies).","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"In this case, \"inserting f(::Float64)\" indicates that we added a method with signature f(::Float64), and that this method triggered invalidations. (Invalidations can also be triggered by method deletion, although this should not happen in typical usage.) Next, notice the backedges line, and the fact that there are two items listed for it. This indicates that there were two proximal triggers for the invalidation, both of which superseded the method f(::Real). One of these had been compiled specifically for Float64, due to our call2f(c64). The other had been compiled specifically for AbstractFloat, due to our call2f(cabs).","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"You can look at these invalidation trees in greater detail:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> method_invalidations = trees[1];    # invalidations stemming from a single method\n\njulia> root = method_invalidations.backedges[1]  # get the first triggered invalidation\nMethodInstance for f(::Float64) at depth 0 with 2 children\n\njulia> show(root)\nMethodInstance for f(::Float64) (2 children)\n MethodInstance for callf(::Vector{Float64}) (1 children)\n ⋮\n\njulia> show(root; minchildren=0)\nMethodInstance for f(::Float64) (2 children)\n MethodInstance for callf(::Vector{Float64}) (1 children)\n  MethodInstance for call2f(::Vector{Float64}) (0 children)","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"The indentation here reveals that call2f called callf which called f, and shows the entire \"chain\" of invalidations triggered by this method definition. Examining root2 = method_invalidations.backedges[2] yields similar results, but for Vector{AbstractFloat}.","category":"page"},{"location":"snoopr/#mt_backedges-invalidations","page":"Snooping on and fixing invalidations: @snoopr","title":"mt_backedges invalidations","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"MethodInvalidations can have a second field, mt_backedges. These are invalidations triggered via the MethodTable for a particular function. When extracting mt_backedges, in addition to a root MethodInstance these also indicate a particular signature that triggered the invalidation. We can illustrate this by returning to the call2f example above:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> call2f([\"hello\"])\nERROR: MethodError: no method matching f(::String)\n[...]\n\njulia> invalidations = @snoopr f(::AbstractString) = 2;\n\njulia> trees = invalidation_trees(invalidations)\n1-element Vector{SnoopCompile.MethodInvalidations}:\n inserting f(::AbstractString) in Main at REPL[6]:1 invalidated:\n   mt_backedges: 1: signature Tuple{typeof(f), String} triggered MethodInstance for callf(::Vector{String}) (1 children)\n\n\njulia> sig, root = trees[1].mt_backedges[end];\n\njulia> sig\nTuple{typeof(f), String}\n\njulia> root\nMethodInstance for callf(::Vector{String}) at depth 0 with 1 children","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"You can see that the invalidating signature, f(::String), is more specific than the signature of the defined method, but that it is what was minimally needed by callf(::Vector{String}).","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"mt_backedges invalidations often reflect \"unhandled\" conditions in methods that have already been compiled.","category":"page"},{"location":"snoopr/#A-more-complex-example","page":"Snooping on and fixing invalidations: @snoopr","title":"A more complex example","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"The structure of these trees can be considerably more complicated. For example, if callf also got called by some other method, and that method had also been executed (forcing it to be compiled), then callf would have multiple children. This is often seen with more complex, real-world tests. As a medium-complexity example, try the following:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"info: Info\nAny demonstration involving real-world packages might be altered from what is shown here by new releases of the relevant packages.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> using Revise\n\njulia> using SnoopCompileCore\n\njulia> invalidations = @snoopr using FillArrays;\n\njulia> using SnoopCompile\n\njulia> trees = invalidation_trees(invalidations)\n3-element Vector{SnoopCompile.MethodInvalidations}:\n inserting all(f::Function, x::FillArrays.AbstractFill) in FillArrays at /pathto/.julia/packages/FillArrays/NjFh2/src/FillArrays.jl:556 invalidated:\n   backedges: 1: superseding all(f::Function, a::AbstractArray; dims) in Base at reducedim.jl:880 with MethodInstance for all(::Base.var\"#388#389\"{_A} where _A, ::AbstractArray) (3 children)\n              2: superseding all(f, itr) in Base at reduce.jl:918 with MethodInstance for all(::Base.var\"#388#389\"{_A} where _A, ::Any) (3 children)\n\n inserting any(f::Function, x::FillArrays.AbstractFill) in FillArrays at /pathto/.julia/packages/FillArrays/NjFh2/src/FillArrays.jl:555 invalidated:\n   backedges: 1: superseding any(f::Function, a::AbstractArray; dims) in Base at reducedim.jl:877 with MethodInstance for any(::typeof(ismissing), ::AbstractArray) (1 children)\n              2: superseding any(f, itr) in Base at reduce.jl:871 with MethodInstance for any(::typeof(ismissing), ::Any) (1 children)\n              3: superseding any(f, itr) in Base at reduce.jl:871 with MethodInstance for any(::LoweredCodeUtils.var\"#11#12\"{_A} where _A, ::Any) (2 children)\n              4: superseding any(f::Function, a::AbstractArray; dims) in Base at reducedim.jl:877 with MethodInstance for any(::LoweredCodeUtils.var\"#11#12\"{_A} where _A, ::AbstractArray) (4 children)\n\n inserting broadcasted(::Base.Broadcast.DefaultArrayStyle{N}, op, r::FillArrays.AbstractFill{T,N,Axes} where Axes) where {T, N} in FillArrays at /pathto/.julia/packages/FillArrays/NjFh2/src/fillbroadcast.jl:8 invalidated:\n   backedges: 1: superseding broadcasted(::S, f, args...) where S<:Base.Broadcast.BroadcastStyle in Base.Broadcast at broadcast.jl:1265 with MethodInstance for broadcasted(::Base.Broadcast.BroadcastStyle, ::typeof(JuliaInterpreter._Typeof), ::Any) (1 children)\n              2: superseding broadcasted(::S, f, args...) where S<:Base.Broadcast.BroadcastStyle in Base.Broadcast at broadcast.jl:1265 with MethodInstance for broadcasted(::Base.Broadcast.BroadcastStyle, ::typeof(string), ::AbstractArray) (177 children)","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Your specific results may differ from this, depending on which version of Julia and of packages you are using. In this case, you can see that three methods (one for all, one for any, and one for broadcasted) triggered invalidations. Perusing this list, you can see that methods in Base, LoweredCodeUtils, and JuliaInterpreter (the latter two were loaded by Revise) got invalidated by methods defined in FillArrays.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"The most consequential ones (the ones with the most children) are listed last, and should be where you direct your attention first. That last entry looks particularly problematic, so let's extract it:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> methinvs = trees[end];\n\njulia> root = methinvs.backedges[end]\nMethodInstance for broadcasted(::Base.Broadcast.BroadcastStyle, ::typeof(string), ::AbstractArray) at depth 0 with 177 children\n\njulia> show(root; maxdepth=10)\nMethodInstance for broadcasted(::Base.Broadcast.BroadcastStyle, ::typeof(string), ::AbstractArray) (177 children)\n MethodInstance for broadcasted(::typeof(string), ::AbstractArray) (176 children)\n  MethodInstance for #unpack#104(::Bool, ::typeof(Pkg.PlatformEngines.unpack), ::String, ::String) (175 children)\n   MethodInstance for (::Pkg.PlatformEngines.var\"#unpack##kw\")(::NamedTuple{(:verbose,),Tuple{Bool}}, ::typeof(Pkg.PlatformEngines.unpack), ::String, ::String) (174 children)\n    MethodInstance for #download_verify_unpack#109(::Nothing, ::Bool, ::Bool, ::Bool, ::Bool, ::typeof(Pkg.PlatformEngines.download_verify_unpack), ::String, ::Nothing, ::String) (165 children)\n     MethodInstance for (::Pkg.PlatformEngines.var\"#download_verify_unpack##kw\")(::NamedTuple{(:ignore_existence, :verbose),Tuple{Bool,Bool}}, ::typeof(Pkg.PlatformEngines.download_verify_unpack), ::String, ::Nothing, ::String) (33 children)\n      MethodInstance for (::Pkg.Artifacts.var\"#39#40\"{Bool,String,Nothing})(::String) (32 children)\n       MethodInstance for create_artifact(::Pkg.Artifacts.var\"#39#40\"{Bool,String,Nothing}) (31 children)\n        MethodInstance for #download_artifact#38(::Bool, ::Bool, ::typeof(Pkg.Artifacts.download_artifact), ::Base.SHA1, ::String, ::Nothing) (30 children)\n         MethodInstance for (::Pkg.Artifacts.var\"#download_artifact##kw\")(::NamedTuple{(:verbose, :quiet_download),Tuple{Bool,Bool}}, ::typeof(Pkg.Artifacts.download_artifact), ::Base.SHA1, ::String, ::Nothing) (23 children)\n          MethodInstance for (::Pkg.Artifacts.var\"#download_artifact##kw\")(::NamedTuple{(:verbose, :quiet_download),Tuple{Bool,Bool}}, ::typeof(Pkg.Artifacts.download_artifact), ::Base.SHA1, ::String) (22 children)\n          ⋮\n        ⋮\n     MethodInstance for (::Pkg.PlatformEngines.var\"#download_verify_unpack##kw\")(::NamedTuple{(:ignore_existence,),Tuple{Bool}}, ::typeof(Pkg.PlatformEngines.download_verify_unpack), ::String, ::Nothing, ::String) (130 children)\n      MethodInstance for (::Pkg.Types.var\"#94#97\"{Pkg.Types.Context,String,Pkg.Types.RegistrySpec})(::String) (116 children)\n       MethodInstance for #mktempdir#21(::String, ::typeof(mktempdir), ::Pkg.Types.var\"#94#97\"{Pkg.Types.Context,String,Pkg.Types.RegistrySpec}, ::String) (115 children)\n        MethodInstance for mktempdir(::Pkg.Types.var\"#94#97\"{Pkg.Types.Context,String,Pkg.Types.RegistrySpec}, ::String) (114 children)\n         MethodInstance for mktempdir(::Pkg.Types.var\"#94#97\"{Pkg.Types.Context,String,Pkg.Types.RegistrySpec}) (113 children)\n          MethodInstance for clone_or_cp_registries(::Pkg.Types.Context, ::Vector{Pkg.Types.RegistrySpec}, ::String) (112 children)\n          ⋮\n     ⋮\n   ⋮","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Here you can see a much more complex branching structure. From this, you can see that methods in Pkg are the most significantly affected; you could expect that loading FillArrays might slow down your next Pkg operation (perhaps depending on which operation you choose) executed in this same session.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Again, if you're following along, it's possible that you'll see something quite different, if subsequent development has protected Pkg against this form of invalidation.","category":"page"},{"location":"snoopr/#Filtering-invalidations","page":"Snooping on and fixing invalidations: @snoopr","title":"Filtering invalidations","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Some method definitions trigger widespread invalidation. If you don't have time to fix all of them, you might want to focus on a specific set of invalidations. For instance, you might be the author of PkgA and you've noted that loading PkgB invalidates a lot of PkgA's code. In that case, you might want to find just those invalidations triggered in your package. You can find them with filtermod:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"trees = invalidation_trees(@snoopr using PkgB)\nftrees = filtermod(PkgA, trees)","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"By default, filtermod only selects trees where the root method was defined in the specified module. filtermod(PkgA, trees; recursive=true) will return all trees that lead to any method defined in PkgA.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"A more selective yet exhaustive tool is findcaller, which allows you to find the path through the trees to a particular method:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"m = @which f(data)                  # look for the \"path\" that invalidates this method\nf(data)                             # run once to force compilation\nusing SnoopCompile\ntrees = invalidation_trees(@snoopr using SomePkg)\ninvs = findcaller(m, trees)         # select the branch that invalidated a compiled instance of `m`","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"When you don't know which method to choose, but know an operation that got slowed down by loading SomePkg, you can use @snoopi to find methods that needed to be recompiled. See findcaller for further details.","category":"page"},{"location":"snoopr/#Fixing-invalidations","page":"Snooping on and fixing invalidations: @snoopr","title":"Fixing invalidations","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"In addition to the text below, there is a video illustrating many of the same package features. The video also walks through a real-world example fixing invalidations that stemmed from inference problems in some of Pkg's code.","category":"page"},{"location":"snoopr/#ascend","page":"Snooping on and fixing invalidations: @snoopr","title":"ascend","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"SnoopCompile, partnering with the remarkable Cthulhu, provides a tool called ascend to simplify diagnosing and fixing invalidations. To demonstrate this tool, let's use it on our test methods defined above. For best results, you'll want to copy those method definitions into a file:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"f(::Real) = 1\ncallf(container) = f(container[1])\ncall2f(container) = callf(container)\n\nc64  = [1.0]; c32 = [1.0f0]; cabs = AbstractFloat[1.0];\ncall2f(c64)\ncall2f(c32)\ncall2f(cabs)\n\nusing SnoopCompileCore\ninvalidations = @snoopr f(::Float64) = 2\nusing SnoopCompile\ntrees = invalidation_trees(invalidations)\nmethod_invalidations = trees[1]","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"and include it into a fresh session.  (The full functionality of ascend doesn't work for methods defined at the REPL, but does if the methods are defined in a file.) In this demo, I called that file /tmp/snoopr.jl.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"We start with","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> root = method_invalidations.backedges[end]\nMethodInstance for f(::AbstractFloat) at depth 0 with 2 children","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"(It's common to start from the last element of backedges or mt_backedges since these have the largest number of children and are therefore most consequential.) Then:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> ascend(root)\nChoose a call for analysis (q to quit):\n >   f(::AbstractFloat)\n       callf(::Vector{AbstractFloat})\n         call2f(::Vector{AbstractFloat})","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"This is an interactive menu: press the down arrow to go down, the up arrow to go up, and Enter to select an item for more detailed analysis. In large trees, you may also want to \"fold\" nodes of the tree (collapsing it so that the children are no longer displayed), particularly if you are working your way through a long series of invalidations and want to hide ones you've already dealt with. You toggle folding using the space bar, and folded nodes are printed with a + in front of them.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"For example, if we press the down arrow once, we get","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> ascend(root)\nChoose a call for analysis (q to quit):\n     f(::AbstractFloat)\n >     callf(::Vector{AbstractFloat})\n         call2f(::Vector{AbstractFloat})","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Now hit Enter to select it:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Choose caller of MethodInstance for f(::AbstractFloat) or proceed to typed code:\n > \"/tmp/snoopr.jl\", callf: lines [2]\n   Browse typed code","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"This is showing you another menu, with only two options (a third is to go back by hitting q). The first entry shows you the option to open the \"offending\" source file in callf at the position of the call to the parent node of callf, which in this case is f. (Sometimes there will be more than one call to the parent within the method, in which case instead of showing [1] it might show [1, 17, 39] indicating each separate location.) Selecting this option, when available, is typically the best way to start because you can sometimes resolve the problem just by inspection of the source.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"If you hit the down arrow","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Choose caller of MethodInstance for f(::AbstractFloat) or proceed to typed code:\n   \"/tmp/snoopr.jl\", callf: lines [2]\n > Browse typed code","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"and then hit Enter, this is what you see:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"│ ─ %-1  = invoke callf(::Vector{AbstractFloat})::Int64\nVariables\n  #self#::Core.Const(callf, false)\n  container::Vector{AbstractFloat}\n\nBody::Int64\n    @ /tmp/snoopr.jl:2 within `callf'\n1 ─ %1 = Base.getindex(container, 1)::AbstractFloat\n│   %2 = Main.f(%1)::Int64\n└──      return %2\n\nSelect a call to descend into or ↩ to ascend. [q]uit. [b]ookmark.\nToggles: [o]ptimize, [w]arn, [d]ebuginfo, [s]yntax highlight for Source/LLVM/Native.\nShow: [S]ource code, [A]ST, [L]LVM IR, [N]ative code\nAdvanced: dump [P]arams cache.\n\n • %1  = invoke getindex(::Vector{AbstractFloat},::Int64)::AbstractFloat\n   %2  = call #f(::AbstractFloat)::Int64\n   ↩","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"This is output from Cthulhu, and you should see its documentation for more information. (See also this video.) While it takes a bit of time to master Cthulhu, it is an exceptionally powerful tool for diagnosing and fixing inference issues.","category":"page"},{"location":"snoopr/#\"Dead-ends\":-finding-runtime-callers-with-MethodAnalysis","page":"Snooping on and fixing invalidations: @snoopr","title":"\"Dead ends\": finding runtime callers with MethodAnalysis","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"When a call is made by runtime dispatch and the world of available methods to handle the call does not narrow the types beyond what is known to the caller, the call-chain terminates. Here is a real-world example (one that may already be \"fixed\" by the time you read this) from analyzing invalidations triggered by specializing Base.unsafe_convert(::Type{Ptr{T}}, ::Base.RefValue{S}) for specific types S and T:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> ascend(root)\nChoose a call for analysis (q to quit):\n >   unsafe_convert(::Type{Ptr{Nothing}}, ::Base.RefValue{_A} where _A)\n       _show_default(::IOBuffer, ::Any)\n         show_default(::IOBuffer, ::Function)\n           show_function(::IOBuffer, ::Function, ::Bool)\n             print(::IOBuffer, ::Function)\n         show_default(::IOBuffer, ::ProcessFailedException)\n           show(::IOBuffer, ::ProcessFailedException)\n             print(::IOBuffer, ::ProcessFailedException)\n         show_default(::IOBuffer, ::Sockets.IPAddr)\n           show(::IOBuffer, ::Sockets.IPAddr)","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Unfortunately for our investigations, none of these \"top level\" callers have defined backedges. (Overall, it's very fortunate that they don't, in that runtime dispatch without backedges avoids any need to invalidate the caller; the alternative would be extremely long chains of completely unnecessary invalidation, which would have many undesirable consequences.)","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"If you want to fix such \"short chains\" of invalidation, one strategy is to identify callers by brute force search enabled by the MethodAnalysis package. For example, one can discover the caller of show(::IOBuffer, ::Sockets.IPAddr) with","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"using MethodAnalysis       # best from a fresh Julia session\nmis = methodinstances();   # collect all *existing* MethodInstances (any future compilation will be ignored)\n# Create a predicate that finds these argument types\nusing Sockets\nargmatch(typs) = length(typs) >= 2 && typs[1] === IOBuffer && typs[2] === Sockets.IPAddr\n# Find any callers\ncallers = findcallers(show, argmatch, mis)","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"which yields a single hit in print(::IOBuffer, ::IPAddr). This too lacks any backedges, so a second application findcallers(print, argmatch, mis) links to print_to_string(::IPAddr). This MethodInstance has a backedge to string(::IPAddr), which has backedges to the method Distributed.connect_to_worker(host::AbstractString, port::Integer). A bit of digging shows that this calls Sockets.getaddrinfo to look up an IP address, and this is inferred to return an IPAddr but the concrete type is unknown. A potential fix for this situation is described below.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"This does not always work; for example, trying something similar for ProcessExitedException fails, likely because the call was made with even less type information. We might be able to find it with a more general predicate, for example","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"argmatch(typs) = length(typs) >= 2 && typs[1] === IOBuffer && ProcessExitedException <: typs[2]","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"but this returns a lot of candidates and it is difficult to guess which of these might be the culprit(s). Finally, findcallers only detects method calls that are \"hard-wired\" into type-inferred code; if the call we're seeking was made from toplevel, or if the function itself was a runtime variable, there is no hope that findcallers will detect it.","category":"page"},{"location":"snoopr/#Tips-for-fixing-invalidations","page":"Snooping on and fixing invalidations: @snoopr","title":"Tips for fixing invalidations","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Invalidations occur in situations like our call2f(c64) example, where we changed our mind about what value f should return for Float64. Julia could not have returned the newly-correct answer without recompiling the call chain.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Aside from cases like these, most invalidations occur whenever new types are introduced, and some methods were previously compiled for abstract types. In some cases, this is inevitable, and the resulting invalidations simply need to be accepted as a consequence of a dynamic, updateable language. (As recommended above, you can often minimize invalidations by loading all your code at the beginning of your session, before triggering the compilation of more methods.) However, in many circumstances an invalidation indicates an opportunity to improve code. In our first example, note that the call call2f(c32) did not get invalidated: this is because the compiler knew all the specific types, and new methods did not affect any of those types. The main tips for writing invalidation-resistant code are:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"use concrete types wherever possible\nwrite inferrable code\ndon't engage in type-piracy (our c64 example is essentially like type-piracy, where we redefined behavior for a pre-existing type)","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Since these tips also improve performance and allow programs to behave more predictably, these guidelines are not intrusive. Indeed, searching for and eliminating invalidations can help you improve the quality of your code.","category":"page"},{"location":"snoopr/#Fixing-Core.Box","page":"Snooping on and fixing invalidations: @snoopr","title":"Fixing Core.Box","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Julia issue 15276 is one of the more surprising forms of inference failure; it is the most common cause of a Core.Box annotation. If other variables depend on the Boxed variable, then a single Core.Box can lead to widespread inference problems. For this reason, these are also among the first inference problems you should tackle.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Read this explanation of why this happens and what you can do to fix it. If you are directed to find Core.Box inference triggers via suggest, you may need to explore around the call site a bit– the inference trigger may be in the closure itself, but the fix needs to go in the method that creates the closure.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Use of ascend is highly recommended for fixing Core.Box inference failures.","category":"page"},{"location":"snoopr/#Adding-type-annotations","page":"Snooping on and fixing invalidations: @snoopr","title":"Adding type annotations","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"In cases where invalidations occur, but you can't use concrete types (there are indeed many valid uses of Vector{Any}), you can often prevent the invalidation using some additional knowledge. One common example is extracting information from an IOContext structure, which is roughly defined as","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"struct IOContext{IO_t <: IO} <: AbstractPipe\n    io::IO_t\n    dict::ImmutableDict{Symbol, Any}\nend","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"There are good reasons to use a value-type of Any, but that makes it impossible for the compiler to infer the type of any object looked up in an IOContext. Fortunately, you can help! For example, the documentation specifies that the :color setting should be a Bool, and since it appears in documentation it's something we can safely enforce. Changing","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"iscolor = get(io, :color, false)","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"to","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"iscolor = get(io, :color, false)::Bool     # assert that the rhs is Bool-valued","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"will throw an error if it isn't a Bool, and this allows the compiler to take advantage of the type being known in subsequent operations.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"We've already seen another relevant example above, where getaddrinfo(::AbstractString) was inferred to return an IPAddr, which is an abstract type. Since there are only two such types supported by the Sockets library, one potential fix is to annotate the returned value from getaddrinfo to be Union{IPv4,IPv6}. This will allow Julia to union-split future operations made using the returned value.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Before turning to a more complex example, it's worth noting that this trick applied to field accesses of abstract types is often one of the simplest ways to fix widespread inference problems. This is such an important case that it is described in the section below.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"As a more detailed example, suppose you're writing code that parses Julia's Expr type:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"julia> ex = :(Array{Float32,3})\n:(Array{Float32, 3})\n\njulia> dump(ex)\nExpr\n  head: Symbol curly\n  args: Vector{Any(3,))\n    1: Symbol Array\n    2: Symbol Float32\n    3: Int64 3","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"ex.args is a Vector{Any}. However, for a :curly expression only certain types will be found among the arguments; you could write key portions of your code as","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"a = ex.args[2]\nif a isa Symbol\n    # inside this block, Julia knows `a` is a Symbol, and so methods called on `a` will be resistant to invalidation\n    foo(a)\nelseif a isa Expr && length((a::Expr).args) > 2\n    a::Expr         # sometimes you have to help inference by adding a type-assert\n    x = bar(a)      # `bar` is now resistant to invalidation\nelseif a isa Integer\n    # even though you've not made this fully-inferrable, you've at least reduced the scope for invalidations\n    # by limiting the subset of `foobar` methods that might be called\n    y = foobar(a)\nend","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Other tricks include replacing broadcasting on v::Vector{Any} with Base.mapany(f, v)–mapany avoids trying to narrow the type of f(v[i]) and just assumes it will be Any, thereby avoiding invalidations of many convert methods.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Adding type-assertions and fixing inference problems are the most common approaches for fixing invalidations. You can discover these manually, but using Cthulhu is highly recommended.","category":"page"},{"location":"snoopr/#Inferrable-field-access-for-abstract-types","page":"Snooping on and fixing invalidations: @snoopr","title":"Inferrable field access for abstract types","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"When invalidations happen for methods that manipulate fields of abstract types, often there is a simple solution: create an \"interface\" for the abstract type specifying that certain fields must have certain types. Here's an example:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"abstract type AbstractDisplay end\n\nstruct Monitor <: AbstractDisplay\n    height::Int\n    width::Int\n    maker::String\nend\n\nstruct Phone <: AbstractDisplay\n    height::Int\n    width::Int\n    maker::Symbol\nend\n\nfunction Base.show(@nospecialize(d::AbstractDisplay), x)\n    str = string(x)\n    w = d.width\n    if length(str) > w  # do we have to truncate to fit the display width?\n        ...","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"In this show method, we've deliberately chosen to prevent specialization on the specific type of AbstractDisplay (to reduce the total number of times we have to compile this method). As a consequence, Julia's inference generally will not realize that d.width returns an Int–it might be able to discover that by exhaustively checking all subtypes, but if there are a lot of such subtypes then such checks would slow compilation considerably.","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Fortunately, you can help by defining an interface for generic AbstractDisplay objects:","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"function Base.getproperty(d::AbstractDisplay, name::Symbol)\n    if name === :height\n        return getfield(d, :height)::Int\n    elseif name === :width\n        return getfield(d, :width)::Int\n    elseif name === :maker\n        return getfield(d, :maker)::Union{String,Symbol}\n    end\n    return getfield(d, name)\nend","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"Julia's constant propagation will ensure that most accesses of those fields will be determined at compile-time, so this simple change robustly fixes many inference problems.","category":"page"},{"location":"snoopr/#Handling-edge-cases","page":"Snooping on and fixing invalidations: @snoopr","title":"Handling edge cases","text":"","category":"section"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"You can sometimes get invalidations from failing to handle \"formal\" possibilities. For example, operations with regular expressions might return a Union{Nothing, RegexMatch}. You can sometimes get poor type inference by writing code that fails to take account of the possibility that nothing might be returned. For example, a comprehension","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"ms = [m.match for m in match.((rex,), my_strings)]","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"might be replaced with","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"ms = [m.match for m in match.((rex,), my_strings) if m !== nothing]","category":"page"},{"location":"snoopr/","page":"Snooping on and fixing invalidations: @snoopr","title":"Snooping on and fixing invalidations: @snoopr","text":"and return a better-typed result.","category":"page"},{"location":"snoopi_deep/#Snooping-on-inference:-@snoopi_deep","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"","category":"section"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"compat: Compat\n@snoopi_deep is available on Julia 1.6.0-DEV.1190 or above, but the results can be relevant for all Julia versions.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"Currently, precompile only caches results for type-inference, not other stages in code generation. For that reason, efforts at reducing latency should be informed by measuring the amount of time spent on type-inference. Moreover, because all code needs to be type-inferred before undergoing later stages of code generation, monitoring this \"entry point\" can give you an overview of the entire compile chain.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"On older versions of Julia, @snoopi allows you to make fairly coarse measurements on inference; starting with Julia 1.6, the recommended tool is @snoopi_deep, which collects a much more detailed picture of type-inference's actions.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"The rich data collected by @snoopi_deep are useful for several different purposes; on this page, we'll describe the basic tool and show how it can be used to profile inference. On later pages we'll show other ways to use the data to reduce the amount of type-inference or cache its results.","category":"page"},{"location":"snoopi_deep/#Collecting-the-data","page":"Snooping on inference: @snoopi_deep","title":"Collecting the data","text":"","category":"section"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"Like @snoopr, @snoopi_deep is exported by both SnoopCompileCore and SnoopCompile, but in this case there is not as much reason to do the data collection by a very minimal package.  Consequently here we'll just load SnoopCompile at the outset.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"To see @snoopi_deep in action, we'll use the following demo:","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"module FlattenDemo\n    struct MyType{T} x::T end\n\n    extract(y::MyType) = y.x\n\n    function domath(x)\n        y = x + x\n        return y*x + 2*x + 5\n    end\n\n    dostuff(y) = domath(extract(y))\n\n    function packintype(x)\n        y = MyType{Int}(x)\n        return dostuff(y)\n    end\nend\n\n# output\n\nFlattenDemo","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"The main call, packintype, stores the input in a struct, and then calls functions that extract the field value and performs arithmetic on the result. To profile inference on this call, we simply do the following:","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"julia> tinf = @snoopi_deep FlattenDemo.packintype(1)\nInferenceTimingNode: 0.00932195/0.010080857 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 1 direct children","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"tip: Tip\nInference gets called only on the first invocation of a method with those specific types. You have to redefine the FlattenDemo module (by just re-executing the command we used to define it) if you want to collect data with @snoopi_deep on the same code a second time.To make it easier to perform these demonstrations and use them for documentation purposes, SnoopCompile includes a function SnoopCompile.flatten_demo() that redefines the module and returns tinf.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"This may not look like much, but there's a wealth of information hidden inside tinf.","category":"page"},{"location":"snoopi_deep/#A-quick-check-for-potential-invalidations","page":"Snooping on inference: @snoopi_deep","title":"A quick check for potential invalidations","text":"","category":"section"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"After running @snoopi_deep, it's generally recommended to check the output of staleinstances:","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"julia> staleinstances(tinf)\nSnoopCompileCore.InferenceTiming[]","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"If you see this, all's well. A non-empty list might indicate method invalidations, which can be checked (in a fresh session) by running the identical workload with @snoopr.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"warning: Warning\nRampant invalidation can make the process of analyzing tinf more confusing: \"why am I getting reinference of this MethodInstance when I precompiled it?\" Routine use of staleinstances at the beginning can save you some head-scratching later.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"tip: Tip\nYour workload may load packages and/or (re)define methods; these can be sources of invalidation and therefore non-empty output from staleinstances. One trick that may cirumvent some invalidation is to load the packages and make the method definitions before launching @snoopi_deep, because it ensures the methods are in place before your workload triggers compilation.","category":"page"},{"location":"snoopi_deep/#Viewing-the-results","page":"Snooping on inference: @snoopi_deep","title":"Viewing the results","text":"","category":"section"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"Let's start unpacking the output of @snoopi_deep and see how to get more insight. First, notice that the output is an InferenceTimingNode: it's the root element of a tree of such nodes, all connected by caller-callee relationships. Indeed, this particular node is for Core.Compiler.Timings.ROOT(), a \"dummy\" node that is the root of all such trees.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"You may have noticed that this ROOT node prints with two numbers. It will be easier to understand their meaning if we first display the whole tree. We can do that with the AbstractTrees package:","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"julia> using AbstractTrees\n\njulia> print_tree(tinf)\nInferenceTimingNode: 0.00932195/0.0100809 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 1 direct children\n└─ InferenceTimingNode: 0.00018625/0.000758907 on InferenceFrameInfo for FlattenDemo.packintype(::Int64) with 2 direct children\n   ├─ InferenceTimingNode: 0.000132252/0.000132252 on InferenceFrameInfo for MyType{Int64}(::Int64) with 0 direct children\n   └─ InferenceTimingNode: 0.000116106/0.000440405 on InferenceFrameInfo for FlattenDemo.dostuff(::MyType{Int64}) with 2 direct children\n      ├─ InferenceTimingNode: 8.4173e-5/0.000161672 on InferenceFrameInfo for FlattenDemo.extract(::MyType{Int64}) with 2 direct children\n      │  ├─ InferenceTimingNode: 4.3641e-5/4.3641e-5 on InferenceFrameInfo for getproperty(::MyType{Int64}, ::Symbol) with 0 direct children\n      │  └─ InferenceTimingNode: 3.3858e-5/3.3858e-5 on InferenceFrameInfo for getproperty(::MyType{Int64}, x::Symbol) with 0 direct children\n      └─ InferenceTimingNode: 0.000162627/0.000162627 on InferenceFrameInfo for FlattenDemo.domath(::Int64) with 0 direct children","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"This tree structure reveals the caller-callee relationships, showing the specific types that were used for each MethodInstance. Indeed, as the calls to getproperty reveal, it goes beyond the types and even shows the results of constant propagation; the getproperty(::MyType{Int64}, x::Symbol) (note x::Symbol instead of just plain ::Symbol) means that the call was getproperty(y, :x), which corresponds to y.x in the definition of extract.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"note: Note\nGenerally we speak of call graphs rather than call trees. But because inference results are cached (a.k.a., we only \"visit\" each node once), we obtain a tree as a depth-first-search of the full call graph.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"You can extract the MethodInstance with","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"julia> Core.MethodInstance(tinf)\nMethodInstance for ROOT()\n\njulia> Core.MethodInstance(tinf.children[1])\nMethodInstance for packintype(::Int64)","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"Each node in this tree is accompanied by a pair of numbers. The first number is the exclusive inference time (in seconds), meaning the time spent inferring the particular MethodInstance, not including the time spent inferring its callees. The second number is the inclusive time, which is the exclusive time plus the time spent on the callees. Therefore, the inclusive time is always at least as large as the exclusive time.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"The ROOT node is a bit different: its exclusive time measures the time spent on all operations except inference. In this case, we see that the entire call took approximately 10ms, of which 9.3ms was spent on activities besides inference. Almost all of that was code-generation, but it also includes the time needed to run the code. Just 0.76ms was needed to run type-inference on this entire series of calls. As you will quickly discover, inference takes much more time on more complicated code.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"We can also display this tree as a flame graph, using the ProfileView package:","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"julia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:10080857))","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"julia> using ProfileView\n\njulia> ProfileView.view(fg)","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"You should see something like this:","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"(Image: flamegraph)","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"Users are encouraged to read the ProfileView documentation to understand how to interpret this, but briefly:","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"the horizontal axis is time (wide boxes take longer than narrow ones), the vertical axis is call depth\nhovering over a box displays the method that was inferred\nleft-clicking on a box causes the full MethodInstance to be printed in your REPL session\nright-clicking on a box opens the corresponding method in your editor\nctrl-click can be used to zoom in\nempty horizontal spaces correspond to activities other than type-inference\nany boxes colored red (there are none in this particular example, but you'll see some later) correspond to non-precompilable MethodInstances, in which the method is owned by one module but the types are from another unrelated module.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"You can explore this flamegraph and compare it to the output from display_tree.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"Finally, flatten, on its own or together with accumulate_by_source, allows you to get an sense for the cost of individual MethodInstances or Methods.","category":"page"},{"location":"snoopi_deep/","page":"Snooping on inference: @snoopi_deep","title":"Snooping on inference: @snoopi_deep","text":"The tools here allow you to get an overview of where inference is spending its time. Sometimes, this information alone is enough to show you how to change your code to reduce latency: perhaps your code is spending a lot of time inferring cases that are not needed in practice and could be simplified. However, most efforts at latency reduction will probably leverage additional tools (described next) that help identify the main opportunities for intervention.","category":"page"},{"location":"snoopc/#macro-snoopc","page":"Snooping on code generation: @snoopc","title":"Snooping on code generation: @snoopc","text":"","category":"section"},{"location":"snoopc/","page":"Snooping on code generation: @snoopc","title":"Snooping on code generation: @snoopc","text":"@snoopc has the advantage of working on any modern version of Julia. It \"snoops\" on the code-generation phase of compilation (the 'c' is a reference to code-generation). Note that while native code is not cached, it nevertheless reveals which methods are being compiled.","category":"page"},{"location":"snoopc/","page":"Snooping on code generation: @snoopc","title":"Snooping on code generation: @snoopc","text":"Note that unlike @snoopi, @snoopc will generate all methods, not just the top-level methods that trigger compilation. (It is redundant to precompile dependent methods, but neither is it harmful.) It is also worth noting that @snoopc requires \"spinning up\" a new Julia process, and so it is a bit slower than @snoopi.","category":"page"},{"location":"snoopc/","page":"Snooping on code generation: @snoopc","title":"Snooping on code generation: @snoopc","text":"Let's demonstrate @snoopc with a snoop script, in this case for the ColorTypes package:","category":"page"},{"location":"snoopc/","page":"Snooping on code generation: @snoopc","title":"Snooping on code generation: @snoopc","text":"using SnoopCompile\n\n### Log the compiles\n# This only needs to be run once (to generate \"/tmp/colortypes_compiles.log\")\n\nSnoopCompile.@snoopc \"/tmp/colortypes_compiles.log\" begin\n    using ColorTypes, Pkg\n    include(joinpath(dirname(dirname(pathof(ColorTypes))), \"test\", \"runtests.jl\"))\nend\n\n### Parse the compiles and generate precompilation scripts\n# This can be run repeatedly to tweak the scripts\n\ndata = SnoopCompile.read(\"/tmp/colortypes_compiles.log\")\n\npc = SnoopCompile.parcel(reverse!(data[2]))\nSnoopCompile.write(\"/tmp/precompile\", pc)","category":"page"},{"location":"snoopc/","page":"Snooping on code generation: @snoopc","title":"Snooping on code generation: @snoopc","text":"As with @snoopi, the \"/tmp/precompile\" folder will now contain a number of *.jl files, organized by package. For each package, you could copy its corresponding *.jl file into the package's src/ directory and include it into the package as described for @snoopi.","category":"page"},{"location":"snoopc/","page":"Snooping on code generation: @snoopc","title":"Snooping on code generation: @snoopc","text":"There are more complete example illustrating potential options in the examples/ directory.","category":"page"},{"location":"snoopc/#Additional-flags","page":"Snooping on code generation: @snoopc","title":"Additional flags","text":"","category":"section"},{"location":"snoopc/","page":"Snooping on code generation: @snoopc","title":"Snooping on code generation: @snoopc","text":"When calling the @snoopc macro, a new julia process is spawned using the function Base.julia_cmd(). Advanced users may want to tweak the flags passed to this process to suit specific needs. This can be done by passing an array of flags of the form [\"--flag1\", \"--flag2\"] as the first argument to the @snoopc macro. For instance, if you want to pass the --project=/path/to/dir flag to the process, to cause the julia process to load the project specified by the path, a snoop script may look like:","category":"page"},{"location":"snoopc/","page":"Snooping on code generation: @snoopc","title":"Snooping on code generation: @snoopc","text":"using SnoopCompile\n\nSnoopCompile.@snoopc [\"--project=/path/to/dir\"] \"/tmp/compiles.csv\" begin\n    # ... statement to snoop on\nend\n\n# ... processing the precompile statements","category":"page"},{"location":"snoopi/#macro-snoopi","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"","category":"section"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"If you can't use @snoopi_deep due to Julia version constraints, the most useful choice is @snoopi, which is available on Julia 1.2 or higher.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Julia can cache inference results, so you can use @snoopi to generate precompile directives for your package. Executing these directives when the package is compiled may reduce compilation (inference) time when the package is used.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Here's a quick demo:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"using SnoopCompile\n\na = rand(Float16, 5)\n\njulia> inf_timing = @snoopi sum(a)\n1-element Array{Tuple{Float64,Core.MethodInstance},1}:\n (0.011293888092041016, MethodInstance for sum(::Array{Float16,1}))","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"We defined the argument a, and then called sum(a) while \"snooping\" on inference. (The i in @snoopi means \"inference.\") The return is a list of \"top level\" methods that got compiled, together with the amount of time spent on inference. In this case it was just a single method, which required approximately 11ms of inference time. (Inferring sum required inferring all the methods that it calls, but these are subsumed into the top level inference of sum itself.) Note that the method that got called,","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> @which sum(a)\nsum(a::AbstractArray) in Base at reducedim.jl:652","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"is much more general (i.e., defined for AbstractArray) than the MethodInstance (defined for Array{Float16,1}). This is because precompilation requires the types of the arguments to specialize the code appropriately.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"The information obtained from @snoopi can be used in several ways, primarily to reduce latency during usage of your package:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"to help you understand which calls take the most inference time\nto help you write precompile directives that run inference on specific calls during package precompilation, so that you don't pay this cost repeatedly each time you use the package\nto help you identify inference problems that prevent successful or comprehensive precompilation","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"If you're starting a project to try to reduce latency in your package, broadly speaking there are two paths you can take:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"you can use SnoopCompile, perhaps together with CompileBot, to automatically generate lists of precompile directives that may reduce latency;\nyou can use SnoopCompile primarily as an analysis tool, and then intervene manually to reduce latency.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Beginners often leap at option 1, but experience shows there are good reasons to consider option 2. To avoid introducing too much complexity early on, we'll defer this discussion to the end of this page, but readers who are serious about reducing latency should be sure to read Understanding precompilation and its limitations.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"note: Note\nBecause invalidations can prevent effective precompilation, developers analyzing their packages with @snoopi are encouraged to use Julia versions (1.6 and higher) that have a lower risk of invalidations in Base and the standard library.","category":"page"},{"location":"snoopi/#pcscripts","page":"Snooping on inference: @snoopi","title":"Precompile scripts","text":"","category":"section"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"You can use @snoopi to come up with a list of precompile-worthy functions. A recommended approach is to write a script that \"exercises\" the functionality you'd like to precompile. One option is to use your package's \"runtests.jl\" file, or you can write a custom script for this purpose. Here's an example for the FixedPointNumbers package:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"using FixedPointNumbers\n\nx = N0f8(0.2)\ny = x + x\ny = x - x\ny = x*x\ny = x/x\ny = Float32(x)\ny = Float64(x)\ny = 0.3*x\ny = x*0.3\ny = 2*x\ny = x*2\ny = x/15\ny = x/8.0","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Save this as a file \"snoopfpn.jl\" and navigate at the Julia REPL to that directory, and then do","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> using SnoopCompile\n\njulia> inf_timing = @snoopi tmin=0.01 include(\"snoopfpn.jl\")\n2-element Array{Tuple{Float64,Core.MethodInstance},1}:\n (0.03108978271484375, MethodInstance for *(::Normed{UInt8,8}, ::Normed{UInt8,8}))\n (0.04189491271972656, MethodInstance for Normed{UInt8,8}(::Float64))","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Here, note the tmin=0.01, which causes any methods that take less than 10ms of inference time to be discarded.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"note: Note\nIf you're testing this, you might get different results depending on the speed of your machine. Moreover, if FixedPointNumbers has already precompiled these method and type combinations–-perhaps by incorporating a precompile file produced by SnoopCompile–-then those methods will be absent. If you want to try this example, dev FixedPointNumbers and disable any _precompile_() call you find.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"You can inspect these results and write your own precompile file, or use the automated tools provided by SnoopCompile.","category":"page"},{"location":"snoopi/#auto","page":"Snooping on inference: @snoopi","title":"Producing precompile directives automatically","text":"","category":"section"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"You can take the output of @snoopi and \"parcel\" it into packages:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> pc = SnoopCompile.parcel(inf_timing)\nDict{Symbol,Array{String,1}} with 1 entry:\n  :FixedPointNumbers => [\"precompile(Tuple{typeof(*),Normed{UInt8,8},Normed{UInt8,8}})\", \"precompile(Tuple{Type{Normed{UInt8,8}},Float64})\"]","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"This splits the calls up into a dictionary, pc, indexed by the package which \"owns\" each call. (In this case there is only one, FixedPointNumbers, but in more complex cases there may be several.) You can then write the results to files:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> SnoopCompile.write(\"/tmp/precompile\", pc)","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"If you look in the /tmp/precompile directory, you'll see one or more files, named by their parent package, that may be suitable for includeing into the package. In this case:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"/tmp/precompile$ cat precompile_FixedPointNumbers.jl\nfunction _precompile_()\n    ccall(:jl_generating_output, Cint, ()) == 1 || return nothing\n    precompile(Tuple{typeof(*),Normed{UInt8,8},Normed{UInt8,8}})\n    precompile(Tuple{Type{Normed{UInt8,8}},Float64})\nend","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"If you copy this file to a precompile.jl file in the src directory, you can incorporate it into the package like this:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"module FixedPointNumbers\n\n# All the usual commands that define the module go here\n\n# ... followed by:\n\ninclude(\"precompile.jl\")\n_precompile_()\n\nend # module FixedPointNumbers","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"The listed method/type combinations should have their inference results cached. Load the package once to precompile it, and then in a fresh Julia session try this:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> using SnoopCompile\n\njulia> inf_timing = @snoopi tmin=0.01 include(\"snoopfpn.jl\")\n0-element Array{Tuple{Float64,Core.MethodInstance},1}","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"The fact that no methods were returned is a sign of success: Julia didn't need to call inference on those methods, because it used the inference results from the cache file.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"note: Note\nSometimes, @snoopi will show method & type combinations that you precompiled. This is a sign that despite your attempts, Julia declined to cache the inference results for those methods. You can either delete those directives from the precompile file, or hope that they will become useful in a future version of Julia. Note that having many \"useless\" precompile directives can slow down precompilation.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"note: Note\nAs you develop your package, it's possible you'll modify or delete some of the methods that appear in your \"precompile.jl\" file. This will not result in an error; by default precompile fails silently. If you want to be certain that your precompile directives don't go stale, preface each with an @assert. Note that this forces you to update your precompile directives as you modify your package, which may or may not be desirable.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"If you find that some precompile directives are ineffective (they appear in a new @snoopi despite being precompiled) and their inference time is substantial, sometimes a bit of manual investigation of the callees can lead to insights. For example, you might be able to introduce a precompile in a dependent package that can mitigate the total time.","category":"page"},{"location":"snoopi/#Producing-precompile-directives-manually","page":"Snooping on inference: @snoopi","title":"Producing precompile directives manually","text":"","category":"section"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"While this \"automated\" approach is often useful, sometimes it makes more sense to inspect the results and write your own precompile directives. For example, for FixedPointNumbers a more elegant and comprehensive precompile file might be","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"function _precompile_()\n    ccall(:jl_generating_output, Cint, ()) == 1 || return nothing\n    for T in (N0f8, N0f16)      # Normed types we want to support\n        for f in (+, -, *, /)   # operations we want to support\n            precompile(Tuple{typeof(f),T,T})\n            for S in (Float32, Float64, Int)   # other number types we want to support\n                precompile(Tuple{typeof(f),T,S})\n                precompile(Tuple{typeof(f),S,T})\n            end\n        end\n        for S in (Float32, Float64)\n            precompile(Tuple{Type{T},S})\n            precompile(Tuple{Type{S},T})\n        end\n    end\nend","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"This covers +, -, *, /, and conversion for various combinations of types. The results from @snoopi can suggest method/type combinations that might be useful to precompile, but often you can generalize its suggestions in useful ways.","category":"page"},{"location":"snoopi/#Analyzing-omitted-methods","page":"Snooping on inference: @snoopi","title":"Analyzing omitted methods","text":"","category":"section"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"There are some method signatures that cannot be precompiled. For example, suppose you have two packages, A and B, that are independent of one another. Then A.f([B.Object(1)]) cannot be precompiled, because A does not know about B.Object, and B does not know about A.f, unless both A and B get included into a third package.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Such problematic method signatures are removed automatically. If you want to be informed about these removals, you can use Julia's logging framework while running parcel:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> using Base.CoreLogging\n\njulia> logger = SimpleLogger(IOBuffer(), CoreLogging.Debug);\n\njulia> pc = with_logger(logger) do\n           SnoopCompile.parcel(inf_timing)\n       end\n\njulia> msgs = String(take!(logger.stream))","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"The omitted method signatures will be logged to the string msgs.","category":"page"},{"location":"snoopi/#Understanding-precompilation-and-its-limitations","page":"Snooping on inference: @snoopi","title":"Understanding precompilation and its limitations","text":"","category":"section"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Suppose your package includes the following method:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"\"\"\"\n    idx = index_midsum(a)\n\nReturn the index of the first item more than \"halfway to the cumulative sum,\"\nmeaning the smallest integer so that `sum(a[begin:idx]) >= sum(a)/2`.\n\"\"\"\nfunction index_midsum(a::AbstractVector)\n    ca = cumsum(vcat(0, a))   # cumulative sum of items in a, starting from 0\n    s = ca[end]               # the sum of all elements\n    return findfirst(x->x >= s/2, ca) - 1  # compensate for inserting 0\nend","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Now, suppose that you'd like to reduce latency in using this method, and you know that an important use case is when a is a Vector{Int}. Therefore, you might precompile it:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> precompile(index_midsum, (Vector{Int},))\ntrue","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"This will cause Julia to infer this method for the given argument types. If you add such statements to your package, it potentially saves your users from having to wait for it to be inferred each time they use your package.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"note: Note\nThe true indicates that Julia was successfully able to find a method supporting this signature and precompile it. Some people put @assert in front of their package's precompile statements–this way, if you delete or modify methods, \"stale\" precompile directives will trigger an error, thus notifying you that they need to be updated.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"But if you execute these lines in the REPL, and then check how well it worked, you might see something like the following:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> using SnoopCompile\n\njulia> tinf = @snoopi index_midsum([1,2,3,4,100])\n3-element Vector{Tuple{Float64, Core.MethodInstance}}:\n (0.00048613548278808594, MethodInstance for cat_similar(::Int64, ::Type, ::Tuple{Int64}))\n (0.010090827941894531, MethodInstance for (::Base.var\"#cat_t##kw\")(::NamedTuple{(:dims,), Tuple{Val{1}}}, ::typeof(Base.cat_t), ::Type{Int64}, ::Int64, ::Vararg{Any, N} where N))\n (0.016659975051879883, MethodInstance for __cat(::Vector{Int64}, ::Tuple{Int64}, ::Tuple{Bool}, ::Int64, ::Vararg{Any, N} where N))","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Even though we'd already said precompile(index_midsum, (Vector{Int},)) in this session, somehow we needed more inference of various concatenation methods. Why does this happen? A detailed investigation (e.g., using Cthulhu or @code_warntype) would reveal that vcat(0, a) is not inferrable \"all the way down,\" and hence the precompile directive couldn't predict everything that was going to be needed.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"No problem, you say: let's just precompile those methods too. The most expensive is the last one. You might not know where __cat is defined, but you can find out with","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> mi = tinf[end][2]    # get the MethodInstance\nMethodInstance for __cat(::Vector{Int64}, ::Tuple{Int64}, ::Tuple{Bool}, ::Int64, ::Vararg{Any, N} where N)\n\njulia> mi.def               # get the Method\n__cat(A, shape::Tuple{Vararg{Int64, M}}, catdims, X...) where M in Base at abstractarray.jl:1599\n\njulia> mi.def.module        # which module was this method defined in?\nBase","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"note: Note\nWhen using @snoopi you might sometimes see entries like MethodInstance for (::SomeModule.var\"#10#12\"{SomeType})(::AnotherModule.AnotherType). These typically correspond to closures/anonymous functions defined with -> or do blocks, but it may not be immediately obvious where these come from. mi.def will show you the file/line number that these are defined on. You can either convert them into named functions to make them easier to precompile, or you can fix inference problems that prevent automatic precompilation (as illustrated below).","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Armed with this knowledge, let's start a fresh session (so that nothing is precompiled yet), and in addition to defining index_midsum and precompiling it, we also execute","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"julia> precompile(Base.__cat, (Vector{Int64}, Tuple{Int64}, Tuple{Bool}, Int, Vararg{Any, N} where N))\ntrue","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Now if you try that tinf = @snoopi index_midsum([1,2,3,4,100]) line, you'll see that the __cat call is omitted, suggesting success.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"However, if you copy both precompile directives into your package source files and then check it with @snoopi again, you may be in for a rude surprise: the __cat precompile directive doesn't \"work.\" That turns out to be because your package doesn't \"own\" that __cat method–the module is Base rather than YourPackage–and because inference cannot determine that it's needed by by index_midsum(::Vector{Int}), Julia doesn't know which *.ji file to use to store its precompiled form.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"How to fix this? Fundamentally, the problem is that vcat call: if we can write index_midsum in a way so that inference succeeds, then all these problems go away. (You can use ascend(mi), where mi was obtained above, to discover that __cat gets called from vcat. See ascend for more information.) It turns out that vcat is inferrable if all the arguments have the same type, so just changing vcat(0, a) to vcat([zero(eltype(a))], a) fixes the problem. (Alternatively, you could make a copy and then use pushfirst!.) In a fresh Julia session:","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"function index_midsum(a::AbstractVector)\n    ca = cumsum(vcat([zero(eltype(a))], a))   # cumulative sum of items in a, starting from 0\n    s = ca[end]               # the sum of all elements\n    return findfirst(x->x >= s/2, ca) - 1  # compensate for inserting 0\nend\n\njulia> precompile(index_midsum, (Vector{Int},))\ntrue\n\njulia> using SnoopCompile\n\njulia> tinf = @snoopi index_midsum([1,2,3,4,100])\nTuple{Float64, Core.MethodInstance}[]","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"Tada! No additional inference was needed, ensuring that your users will not suffer any latency due to type-inference of this particular method/argument combination. In addition to identifing a call deserving of precompilation, @snoopi helped us identify a weakness in its implementation. Fixing that weakness reduced latency, made the code more resistant to invalidation, and may improve runtime performance.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"In other cases, manual inspection of the results from @snoopi may lead you in a different direction: you may discover that a huge number of specializations are being created for a method that doesn't need them. Typical examples are methods that take types or functions as inputs: for example, there is no reason to recompile methods(f) for each separate f. In such cases, by far your best option is to add @nospecialize annotations to one or more of the arguments of that method. Such changes can have dramatic impact on the latency of your package.","category":"page"},{"location":"snoopi/","page":"Snooping on inference: @snoopi","title":"Snooping on inference: @snoopi","text":"The ability to make interventions like these–which can both reduce latency and improve runtime speed–is a major reason to consider @snoopi primarily as an analysis tool rather than just a utility to blindly generate lists of precompile directives.","category":"page"},{"location":"userimg/#userimg","page":"Creating userimg.jl files","title":"Creating userimg.jl files","text":"","category":"section"},{"location":"userimg/","page":"Creating userimg.jl files","title":"Creating userimg.jl files","text":"If you want to save more precompile information, one option is to create a \"userimg.jl\" file with which to build Julia. This is only supported for @snoopc. Instead of calling SnoopCompile.parcel and SnoopCompile.write, use the following:","category":"page"},{"location":"userimg/","page":"Creating userimg.jl files","title":"Creating userimg.jl files","text":"# Use these two lines if you want to add to your userimg.jl\npc = SnoopCompile.format_userimg(reverse!(data[2]))\nSnoopCompile.write(\"/tmp/userimg_Images.jl\", pc)","category":"page"},{"location":"userimg/","page":"Creating userimg.jl files","title":"Creating userimg.jl files","text":"Now move the resulting file to your Julia source directory, and create a userimg.jl file that includes all the package-specific precompile files you want. Then build Julia from source. You should note that your latencies decrease substantially.","category":"page"},{"location":"userimg/","page":"Creating userimg.jl files","title":"Creating userimg.jl files","text":"There are serious negatives associated with a userimg.jl script:","category":"page"},{"location":"userimg/","page":"Creating userimg.jl files","title":"Creating userimg.jl files","text":"Your julia build times become very long\nPkg.update() will have no effect on packages that you've built into julia until you next recompile julia itself. Consequently, you may not get the benefit of enhancements or bug fixes.\nFor a package that you sometimes develop, this strategy is very inefficient, because testing a change means rebuilding Julia as well as your package.","category":"page"},{"location":"userimg/","page":"Creating userimg.jl files","title":"Creating userimg.jl files","text":"A process similar to this one is also performed via PackageCompiler.","category":"page"},{"location":"snoopi_deep_parcel/#Using-@snoopi_deep-results-to-generate-precompile-directives","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"","category":"section"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"Improving inferrability, specialization, and precompilability may sometimes feel like \"eating your vegetables\": really good for you, but it sometimes feels like work.  (Depending on tastes, of course; I love vegetables.) While we've already gotten some payoff, now we're going to collect an additional reward for our hard work: the \"dessert\" of adding precompile directives. It's worth emphasing that if we hadn't done the analysis of inference triggers and made improvements to our package, the benefit of adding precompile directives would have been substantially smaller.","category":"page"},{"location":"snoopi_deep_parcel/#Parcel","page":"Using @snoopi_deep results to generate precompile directives","title":"Parcel","text":"","category":"section"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"precompile directives have to be emitted by the module that owns the method. SnoopCompile comes with a tool, parcel, that splits out the \"root-most\" precompilable MethodInstances into their constituent modules. In our case, since we've made almost every call precompilable, this will typically correspond to the bottom row of boxes in the flame graph. In cases where you have some non-precompilable MethodInstances, they will include MethodInstances from higher up in the call tree.","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"Let's use SnoopCompile.parcel on OptimizeMeFixed in its current state:","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"julia> ttot, pcs = SnoopCompile.parcel(tinf);\n\njulia> ttot\n0.6084431670000001\n\njulia> pcs\n4-element Vector{Pair{Module, Tuple{Float64, Vector{Tuple{Float64, Core.MethodInstance}}}}}:\n                 Core => (0.000135179, [(0.000135179, MethodInstance for (NamedTuple{(:sizehint,), T} where T<:Tuple)(::Tuple{Int64}))])\n                 Base => (0.028383533000000002, [(3.2456e-5, MethodInstance for getproperty(::IOBuffer, ::Symbol)), (4.7474e-5, MethodInstance for ==(::Type, ::Nothing)), (5.7944e-5, MethodInstance for typeinfo_eltype(::Type)), (0.00039092299999999994, MethodInstance for show(::IOContext{IOBuffer}, ::Any)), (0.000433143, MethodInstance for IOContext(::IOBuffer, ::IOContext{Base.TTY})), (0.000484984, MethodInstance for Pair{Symbol, DataType}(::Any, ::Any)), (0.000742383, MethodInstance for print(::IOContext{Base.TTY}, ::String, ::String, ::Vararg{String, N} where N)), (0.001293705, MethodInstance for Pair(::Symbol, ::Type)), (0.0018914350000000003, MethodInstance for show(::IOContext{IOBuffer}, ::UInt16)), (0.010604793000000001, MethodInstance for show(::IOContext{IOBuffer}, ::Tuple{String, Int64})), (0.012404293, MethodInstance for show(::IOContext{IOBuffer}, ::Vector{Int64}))])\n             Base.Ryu => (0.15733664599999997, [(0.05721630600000001, MethodInstance for writeshortest(::Vector{UInt8}, ::Int64, ::Float32, ::Bool, ::Bool, ::Bool, ::Int64, ::UInt8, ::Bool, ::UInt8, ::Bool, ::Bool)), (0.10012033999999997, MethodInstance for show(::IOContext{IOBuffer}, ::Float32))])\n Main.OptimizeMeFixed => (0.4204474180000001, [(0.4204474180000001, MethodInstance for main())])","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"This tells us that a total of ~0.6s were spent on inference. parcel discovered precompilable MethodInstances for four modules, Core, Base, Base.Ryu, and OptimizeMeFixed. These are listed in increasing order of inference time.","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"Let's look specifically at OptimizeMeFixed, since that's under our control:","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"julia> pcmod = pcs[end]\nMain.OptimizeMeFixed => (0.4204474180000001, Tuple{Float64, Core.MethodInstance}[(0.4204474180000001, MethodInstance for main())])\n\njulia> tmod, tpcs = pcmod.second;\n\njulia> tmod\n0.4204474180000001\n\njulia> tpcs\n1-element Vector{Tuple{Float64, Core.MethodInstance}}:\n (0.4204474180000001, MethodInstance for main())","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"0.42s of that time is due to OptimizeMeFixed, and parcel discovered a single MethodInstances to precompile, main().","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"We could look at the other modules (packages) similarly.","category":"page"},{"location":"snoopi_deep_parcel/#SnoopCompile.write","page":"Using @snoopi_deep results to generate precompile directives","title":"SnoopCompile.write","text":"","category":"section"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"You can generate files that contain ready-to-use precompile directives using SnoopCompile.write:","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"julia> SnoopCompile.write(\"/tmp/precompiles_OptimizeMe\", pcs)\nCore: no precompile statements out of 0.000135179\nBase: precompiled 0.026194226 out of 0.028383533000000002\nBase.Ryu: precompiled 0.15733664599999997 out of 0.15733664599999997\nMain.OptimizeMeFixed: precompiled 0.4204474180000001 out of 0.4204474180000001","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"You'll now find a directory /tmp/precompiles_OptimizeMe, and inside you'll find three files, for Base, Base.Ryu, and OptimizeMeFixed, respectively. The contents of the last of these should be recognizable:","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"function _precompile_()\n    ccall(:jl_generating_output, Cint, ()) == 1 || return nothing\n    Base.precompile(Tuple{typeof(main)})   # time: 0.4204474\nend","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"The first ccall line ensures we only pay the cost of running these precompile directives if we're building the package; this is relevant mostly if you're running Julia with --compiled-modules=no so it is rarely something that matters. (It would also matter if you've set __precompile__(false) at the top of your module, but if so why are you reading this?)","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"This file is ready to be moved into the OptimizeMe repository and included into your module definition. Since we added warmup manually, you could consider moving precompile(warmup, ()) into this function.","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"In general, it's recommended to run precompilation from inside a block","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"if Base.VERSION >= v\"1.4.2\"\n    include(\"precompile.jl\")\n    _precompile_()\nend","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"because earlier versions of Julia occasionally crashed on certain precompile directives. It's also perfectly fine to omit the function call, and use","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"if Base.VERSION >= v\"1.4.2\"\n    Base.precompile(Tuple{typeof(main)})   # time: 0.4204474\n    precompile(warmup, ())\nend","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"directly in the OptimizeMeFixed module, usually as the last block of the module definition.","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"You might also consider submitting some of the other files (or their precompile directives) to the packages you depend on. In some cases, the specific argument type combinations may be too \"niche\" to be worth specializing; one such case is found here, a show method for Tuple{String, Int64} for Base. But in other cases, these may be very worthy additions to the package.","category":"page"},{"location":"snoopi_deep_parcel/#Final-results","page":"Using @snoopi_deep results to generate precompile directives","title":"Final results","text":"","category":"section"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"Let's check out the final results of adding these precompile directives to OptimizeMeFixed. First, let's build both modules as precompiled packages:","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"ulia> push!(LOAD_PATH, \".\")\n4-element Vector{String}:\n \"@\"\n \"@v#.#\"\n \"@stdlib\"\n \".\"\n\njulia> using OptimizeMe\n[ Info: Precompiling OptimizeMe [top-level]\n\njulia> using OptimizeMeFixed\n[ Info: Precompiling OptimizeMeFixed [top-level]","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"Now in fresh sessions,","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"julia> @time (using OptimizeMe; OptimizeMe.main())\n3.14 is great\n2.718 is jealous\n⋮\nObject x: 7\n  3.159908 seconds (10.63 M allocations: 582.091 MiB, 5.19% gc time, 99.67% compilation time)","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"versus","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"julia> @time (using OptimizeMeFixed; OptimizeMeFixed.main())\n3.14 is great\n2.718 is jealous\n⋮\n Object x: 7\n  1.840034 seconds (5.38 M allocations: 289.402 MiB, 5.03% gc time, 96.70% compilation time)","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"We've cut down on the latency by nearly a factor of two. Moreover, if Julia someday caches generated code, we're well-prepared to capitalize on the benefits, because the same improvements in \"code ownership\" are almost certain to pay dividends there too.","category":"page"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"If you inspect the results, you may sometimes suffer a few disappointments: some methods that we expected to precompile don't \"take.\" At the moment there appears to be a small subset of methods that fail to precompile, and the reasons are not yet widely understood. At present, the best advice seems to be to comment-out any precompile directives that don't \"take,\" since otherwise they increase the build time for the package without material benefit. These failures may be addressed in future versions of Julia. It's also worth appreciating how much we have succeeded in reducing latency, with the awareness that we may be able to get even greater benefit in the future.","category":"page"},{"location":"snoopi_deep_parcel/#Summary","page":"Using @snoopi_deep results to generate precompile directives","title":"Summary","text":"","category":"section"},{"location":"snoopi_deep_parcel/","page":"Using @snoopi_deep results to generate precompile directives","title":"Using @snoopi_deep results to generate precompile directives","text":"@snoopi_deep collects enough data to learn which methods are triggering inference, how heavily methods are being specialized, and so on. Examining your code from the standpoint of inference and specialization may be unfamiliar at first, but like other aspects of package development (testing, documentation, and release compatibility management) it can lead to significant improvements in the quality-of-life for you and your users. By optimizing your packages and then adding precompile directives, you can often cut down substantially on latency.","category":"page"},{"location":"pgdsgui/#pgds","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"","category":"section"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"As indicated in the workflow, one of the important early steps is to evaluate and potentially adjust method specialization. Each specialization (each MethodInstance with different argument types) costs extra inference and code-generation time, so while specialization often improves runtime performance, that has to be weighed against the cost in latency. There are also cases in which overspecialization can hurt both run-time and compile-time performance. Consequently, an analysis of specialization can be a powerful tool for improving package quality.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"SnoopCompile ships with an interactive tool, pgdsgui, short for \"Profile-guided despecialization.\" The name is a reference to a related technique, profile-guided optimization (PGO). Both PGO and PGDS use rutime profiling to help guide decisions about code optimization. PGO is often used in languages whose default mode is to avoid specialization, whereas PGDS seems more appropriate for a language like Julia which specializes by default. While PGO is sometimes an automatic part of the compiler that optimizes code midstream during execution, PGDS is a tool for making static changes in code. Again, this seems appropriate for a language where specialization typically happens prior to the first execution of the code.","category":"page"},{"location":"pgdsgui/#Using-the-PGDS-graphical-user-interface","page":"Profile-guided despecialization","title":"Using the PGDS graphical user interface","text":"","category":"section"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"To illustrate the use of PGDS, we'll examine an example in which some methods get specialized for hundreds of types. To keep this example short, we'll create functions that operate on types themselves.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"note: Note\nFor a DataType T, T.name returns a Core.TypeName, and T.name.name returns the name as a Symbol. Base.unwrap_unionall(T) preserves DataTypes as-is, but converts a UnionAll type into a DataType.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"\"\"\"\n    spelltype(T::Type)\n\nSpell out a type's name, one character at a time.\n\"\"\"\nfunction spelltype(::Type{T}) where T\n    name = Base.unwrap_unionall(T).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend\n\n\"\"\"\n    mappushes!(f, dest, src)\n\nLike `map!` except it grows `dest` by one for each element in `src`.\n\"\"\"\nfunction mappushes!(f, dest, src)\n    for item in src\n        push!(dest, f(item))\n    end\n    return dest\nend\n\nmappushes(f, src) = mappushes!(f, [], src)","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"There are two stages to PGDS: first (and preferrably starting in a fresh Julia session), we profile type-inference:","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> using SnoopCompile\n\njulia> Ts = subtypes(Any);  # get a long list of different types\n\njulia> tinf = @snoopi_deep mappushes(spelltype, Ts)\nInferenceTimingNode: 4.476700/5.591207 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 587 direct children","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Then, in the same session, profile the runtime:","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> using Profile\n\njulia> @profile mappushes(spelltype, Ts);","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Typically, it's best if the workload here is reflective of a \"real\" workload (test suites often are not), so that you get a realistic view of where your code spends its time during actual use.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Now let's launch the PDGS GUI:","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> import PyPlot        # the GUI is dependent on PyPlot, must load it before the next line\n\njulia> mref, ax = pgdsgui(tinf);","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"You should see something like this:","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"(Image: pgdsgui)","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In this graph, each dot corresponds to a single method; for this method, we plot inference time (vertical axis) against the run time (horizontal axis). The coloration of each dot encodes the number of specializations (the number of distinct MethodInstances) for that method; by default it even includes the number of times the method was inferred for specific constants (constant propagation), although you can exclude those cases using the consts=false keyword. Finally, the edge of each dot encodes the fraction of time spent on runtime dispatch (aka, type-instability), with black indicating 0% and bright red indicating 100%.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In this plot, we can see that no method runs for more than 0.01 seconds, whereas some methods have an aggregate inference time of up to 1s. Overall, inference-time dominates this plot. Moreover, for the most expensive cases, the number of specializations is in the hundreds or thousands.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"To learn more about what is being specialized, just click on one of the dots; if you choose the upper-left dot (the one with highest inference time), you should see something like this in your REPL:","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"spelltype(::Type{T}) where T in Main at REPL[1]:6 (586 specializations)","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"This tells you the method corresponding to this dot. Moreover, mref (one of the outputs of pgdsgui) holds this method:","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> mref[]\nspelltype(::Type{T}) where T in Main at REPL[1]:6","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"What are the specializations, and how costly was each?","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> collect_for(mref[], tinf)\n586-element Vector{SnoopCompileCore.InferenceTimingNode}:\n InferenceTimingNode: 0.003486/0.020872 on InferenceFrameInfo for spelltype(::Type{T}) where T with 7 direct children\n InferenceTimingNode: 0.003281/0.003892 on InferenceFrameInfo for spelltype(::Type{AbstractArray}) with 2 direct children\n InferenceTimingNode: 0.003349/0.004023 on InferenceFrameInfo for spelltype(::Type{AbstractChannel}) with 2 direct children\n InferenceTimingNode: 0.000827/0.001154 on InferenceFrameInfo for spelltype(::Type{AbstractChar}) with 5 direct children\n InferenceTimingNode: 0.003326/0.004070 on InferenceFrameInfo for spelltype(::Type{AbstractDict}) with 2 direct children\n InferenceTimingNode: 0.000833/0.001159 on InferenceFrameInfo for spelltype(::Type{AbstractDisplay}) with 5 direct children\n⋮\n InferenceTimingNode: 0.000848/0.001160 on InferenceFrameInfo for spelltype(::Type{YAML.Span}) with 5 direct children\n InferenceTimingNode: 0.000838/0.001148 on InferenceFrameInfo for spelltype(::Type{YAML.Token}) with 5 direct children\n InferenceTimingNode: 0.000833/0.001150 on InferenceFrameInfo for spelltype(::Type{YAML.TokenStream}) with 5 direct children\n InferenceTimingNode: 0.000809/0.001126 on InferenceFrameInfo for spelltype(::Type{YAML.YAMLDocIterator}) with 5 direct children","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"So we can see that one MethodInstance for each type in Ts was generated.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"If you see a list of MethodInstances, and the first is extremely costly in terms of inclusive time, but all the rest are not, then you might not need to worry much about over-specialization: your inference time will be dominated by that one costly method (often, the first time the method was called), and the fact that lots of additional specializations were generated may not be anything to worry about. However, in this case, the distribution of time is fairly flat, each contributing a small portion to the overall time. In such cases, over-specialization may be a problem.","category":"page"},{"location":"pgdsgui/#Reducing-specialization-with-@nospecialize","page":"Profile-guided despecialization","title":"Reducing specialization with @nospecialize","text":"","category":"section"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"How might we change this? To reduce the number of specializations of spelltype, we use @nospecialize in its definition:","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function spelltype(@nospecialize(T::Type))\n    name = Base.unwrap_unionall(T).name.name\n    str = \"\"\n    for c in string(name)\n        str *= c\n    end\n    return str\nend","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"warning: Warning\nwhere type-parameters force specialization, regardless of @nospecialize: in spelltype(@nospecialize(::Type{T})) where T, the @nospecialize has no impact and you'll get full specialization on T. Instead, use @nospecialize(T::Type) as shown.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"If we now rerun that demo, you should see a plot of the same kind as shown above, but with different costs for each dot. The differences are best appreciated comparing them side-by-side (pgdsgui allows you to specify a particular axis into which to plot):","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"(Image: pgdsgui-compare)","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"The results with @nospecialize are shown on the right. You can see that:","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Now, the most expensive-to-infer method is <0.01s (formerly it was ~1s)\nNo method has more than 2 specializations","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"Moreover, our runtimes (post-compilation) really aren't very different, both in the ballpark of a few millseconds (you can check with @btime from BenchmarkTools to be sure).","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In total, we've reduced compilation time approximately 50× without appreciably hurting runtime perfomance. Reducing specialization, when appropriate, can often yield your biggest reductions in latency.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"tip: Tip\nWhen you add @nospecialize, sometimes it's beneficial to compensate for the loss of inferrability by adding some type assertions. This topic will be discussed in greater detail in the next section, but for the example above we can improve runtime performance by annotating the return type of Base.unwrap_unionall(T): name = (Base.unwrap_unionall(T)::DataType).name.name. Then, later lines in spell know that name is a Symbol.With this change, the unspecialized variant outperforms the specialized variant in both compile-time and run-time. The reason is that the specialized variant of spell needs to be called by runtime dispatch, whereas for the unspecialized variant there's only one MethodInstance, so its dispatch is handled at compile time.","category":"page"},{"location":"pgdsgui/#Argument-standardization","page":"Profile-guided despecialization","title":"Argument standardization","text":"","category":"section"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"While not immediately relevant to the example above, a very important technique that falls within the domain of reducing specialization is argument standardization: instead of","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function foo(x, y)\n    # some huge function, slow to compile, and you'd prefer not to compile it many times for different types of x and y\nend","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"consider whether you can safely write this as","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"function foo(x::X, y::Y)   # X and Y are concrete types\n    # some huge function, but the concrete typing ensures you only compile it once\nend\nfoo(x, y) = foo(convert(X, x)::X, convert(Y, y)::Y)   # this allows you to still call it with any argument types","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"The \"standardizing method\" foo(x, y) is short and therefore quick to compile, so it doesn't really matter if you compile many different instances.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"tip: Tip\nIn convert(X, x)::X, the final ::X guards against a broken convert method that fails to return an object of type X. Without it, foo(x, y) might call itself in an infinite loop, ultimately triggering a StackOverflowError. StackOverflowErrors are a particularly nasty form of error, and the typeassert ensures that you get a simple TypeError instead.In other contexts, such typeasserts would also have the effect of fixing inference problems even if the type of x is not well-inferred (this will be discussed in more detail later), but in this case dispatch to foo(x::X, y::Y) would have ensured the same outcome.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"There are of course cases where you can't implement your code in this way: after all, part of the power of Julia is the ability of generic methods to \"do the right thing\" for a wide variety of types. But in cases where you're doing a standard task, e.g., writing some data to a file, there's really no good reason to recompile your save method for a filename encoded as a String and again for a SubString{String} and again for a SubstitutionString and again for an AbstractString and ...: after all, the core of the save method probably isn't sensitive to the precise encoding of the filename.  In such cases, it should be safe to convert all filenames to String, thereby reducing the diversity of input arguments for expensive-to-compile methods.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"If you're using pgdsgui, the cost of inference and the number of specializations may guide you to click on specific dots; collect_for(mref[], tinf) then allows you to detect and diagnose cases where argument standardization might be helpful.","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"You can do the same analysis without pgdsgui. The opportunity for argument standardization is often facilitated by looking at, e.g.,","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"julia> tms = accumulate_by_source(flatten(tinf));  # collect all MethodInstances that belong to the same Method\n\njulia> t, m = tms[end-1]        # the ones towards the end take the most time, maybe they are over-specialized?\n(0.4138147, save(filename::AbstractString, data) in SomePkg at /pathto/SomePkg/src/SomePkg.jl:23)\n\njulia> methodinstances(m)       # let's see what specializations we have\n7-element Vector{Core.MethodInstance}:\n MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::SubString{String}, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::AbstractString, ::Vector{SomePkg.SomeDataType})\n MethodInstance for save(::String, ::Vector{SomePkg.SomeDataType{SubString{String}}})\n MethodInstance for save(::SubString{String}, ::Array)\n MethodInstance for save(::String, ::Vector{var\"#s92\"} where var\"#s92\"<:SomePkg.SomeDataType)\n MethodInstance for save(::String, ::Array)","category":"page"},{"location":"pgdsgui/","page":"Profile-guided despecialization","title":"Profile-guided despecialization","text":"In this case we have 7 MethodInstances (some of which are clearly due to poor inferrability of the caller) when one might suffice.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Data-collection","page":"Reference","title":"Data collection","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"@snoopr\n@snoopi_deep\n@snoopi\n@snoopc","category":"page"},{"location":"reference/#SnoopCompileCore.@snoopr","page":"Reference","title":"SnoopCompileCore.@snoopr","text":"list = @snoopr expr\n\nCapture method cache invalidations triggered by evaluating expr. list is a sequence of invalidated Core.MethodInstances together with \"explanations,\" consisting of integers (encoding depth) and strings (documenting the source of an invalidation).\n\nUnless you are working at a low level, you essentially always want to pass list directly to SnoopCompile.invalidation_trees.\n\nExtended help\n\nlist is in a format where the \"reason\" comes after the items. Method deletion results in the sequence\n\n[zero or more (mi, \"invalidate_mt_cache\") pairs..., zero or more (depth1 tree, loctag) pairs..., method, loctag] with loctag = \"jl_method_table_disable\"\n\nwhere mi means a MethodInstance. depth1 means a sequence starting at depth=1.\n\nMethod insertion results in the sequence\n\n[zero or more (depth0 tree, sig) pairs..., same info as with delete_method except loctag = \"jl_method_table_insert\"]\n\n\n\n\n\n","category":"macro"},{"location":"reference/#SnoopCompileCore.@snoopi_deep","page":"Reference","title":"SnoopCompileCore.@snoopi_deep","text":"tinf = @snoopi_deep commands\n\nProduce a profile of julia's type inference, recording the amount of time spent inferring every MethodInstance processed while executing commands. Each fresh entrance to type inference (whether executed directly in commands or because a call was made by runtime-dispatch) also collects a backtrace so the caller can be identified.\n\ntinf is a tree, each node containing data on a particular inference \"frame\" (the method, argument-type specializations, parameters, and even any constant-propagated values). Each reports the exclusive/inclusive times, where the exclusive time corresponds to the time spent inferring this frame in and of itself, whereas the inclusive time includes the time needed to infer all the callees of this frame.\n\nThe top-level node in this profile tree is ROOT. Uniquely, its exclusive time corresponds to the time spent not in julia's type inference (codegen, llvm_opt, runtime, etc).\n\nThere are many different ways of inspecting and using the data stored in tinf. The simplest is to load the AbstracTrees package and display the tree with AbstractTrees.print_tree(tinf). See also:  flamegraph, flatten, inference_triggers, SnoopCompile.parcel, runtime_inferencetime.\n\nExample\n\njulia> tinf = @snoopi_deep begin\n           sort(rand(100))  # Evaluate some code and profile julia's type inference\n       end\nInferenceTimingNode: 0.110018224/0.131464476 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 2 direct children\n\n\n\n\n\n","category":"macro"},{"location":"reference/#SnoopCompileCore.@snoopi","page":"Reference","title":"SnoopCompileCore.@snoopi","text":"inf_timing = @snoopi commands\ninf_timing = @snoopi tmin=0.0 commands\n\nExecute commands while snooping on inference. Returns an array of (t, linfo) tuples, where t is the amount of time spent inferring linfo (a MethodInstance).\n\nMethods that take less time than tmin will not be reported.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#SnoopCompileCore.@snoopc","page":"Reference","title":"SnoopCompileCore.@snoopc","text":"@snoopc \"compiledata.csv\" begin\n    # Commands to execute, in a new process\nend\n\ncauses the julia compiler to log all functions compiled in the course of executing the commands to the file \"compiledata.csv\". This file can be used for the input to SnoopCompile.read.\n\n\n\n\n\n","category":"macro"},{"location":"reference/#GUIs","page":"Reference","title":"GUIs","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"flamegraph\npgdsgui","category":"page"},{"location":"reference/#SnoopCompile.flamegraph","page":"Reference","title":"SnoopCompile.flamegraph","text":"flamegraph(tinf::InferenceTimingNode; tmin=0.0, excluded_modules=Set([Main]), mode=nothing)\n\nConvert the call tree of inference timings returned from @snoopi_deep into a FlameGraph. Returns a FlameGraphs.FlameGraph structure that represents the timing trace recorded for type inference.\n\nFrames that take less than tmin seconds of inclusive time will not be included in the resultant FlameGraph (meaning total time including it and all of its children). This can be helpful if you have a very big profile, to save on processing time.\n\nNon-precompilable frames are marked in reddish colors. excluded_modules can be used to mark methods defined in modules to which you cannot or do not wish to add precompiles.\n\nmode controls how frames are named in tools like ProfileView. nothing uses the default of just the qualified function name, whereas supplying mode=Dict(method => count) counting the number of specializations of each method will cause the number of specializations to be included in the frame name.\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoopi_deep on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.002148974/0.002767166 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 1 direct children\n\njulia> fg = flamegraph(tinf)\nNode(FlameGraphs.NodeData(ROOT() at typeinfer.jl:75, 0x00, 0:3334431))\n\njulia> ProfileView.view(fg);  # Display the FlameGraph in a package that supports it\n\nYou should be able to reconcile the resulting flamegraph to print_tree(tinf) (see flatten).\n\nThe empty horizontal periods in the flamegraph correspond to times when something other than inference is running. The total width of the flamegraph is set from the ROOT node.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.pgdsgui","page":"Reference","title":"SnoopCompile.pgdsgui","text":"methodref, ax = pgdsgui(tinf::InferenceTimingNode; consts::Bool=true, by=inclusive)\nmethodref     = pgdsgui(ax, tinf::InferenceTimingNode; kwargs...)\n\nCreate a scatter plot comparing:     - (vertical axis) the inference time for all instances of each Method, as captured by tinf;     - (horizontal axis) the run time cost, as estimated by capturing a @profile before calling this function.\n\nEach dot corresponds to a single method. The face color encodes the number of times that method was inferred, and the edge color corresponds to the fraction of the runtime spent on runtime dispatch (black is 0%, bright red is 100%). Clicking on a dot prints the method (or location, if inlined) to the REPL, and sets methodref[] to that method.\n\nax is the pyplot axis of the scatterplot.\n\ncompat: Compat\npgdsgui depends on PyPlot via the Requires.jl package. You must load both SnoopCompile and PyPlot for this function to be defined.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Analysis-of-invalidations","page":"Reference","title":"Analysis of invalidations","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"uinvalidated\ninvalidation_trees\nfiltermod\nfindcaller","category":"page"},{"location":"reference/#SnoopCompile.uinvalidated","page":"Reference","title":"SnoopCompile.uinvalidated","text":"umis = uinvalidated(invlist)\n\nReturn the unique invalidated MethodInstances. invlist is obtained from SnoopCompileCore.@snoopr. This is similar to filtering for MethodInstances in invlist, except that it discards any tagged \"invalidate_mt_cache\". These can typically be ignored because they are nearly inconsequential: they do not invalidate any compiled code, they only transiently affect an optimization of runtime dispatch.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.invalidation_trees","page":"Reference","title":"SnoopCompile.invalidation_trees","text":"trees = invalidation_trees(list)\n\nParse list, as captured by SnoopCompileCore.@snoopr, into a set of invalidation trees, where parents nodes were called by their children.\n\nExample\n\njulia> f(x::Int)  = 1\nf (generic function with 1 method)\n\njulia> f(x::Bool) = 2\nf (generic function with 2 methods)\n\njulia> applyf(container) = f(container[1])\napplyf (generic function with 1 method)\n\njulia> callapplyf(container) = applyf(container)\ncallapplyf (generic function with 1 method)\n\njulia> c = Any[1]\n1-element Array{Any,1}:\n 1\n\njulia> callapplyf(c)\n1\n\njulia> trees = invalidation_trees(@snoopr f(::AbstractFloat) = 3)\n1-element Array{SnoopCompile.MethodInvalidations,1}:\n inserting f(::AbstractFloat) in Main at REPL[36]:1 invalidated:\n   mt_backedges: 1: signature Tuple{typeof(f),Any} triggered MethodInstance for applyf(::Array{Any,1}) (1 children) more specific\n\nSee the documentation for further details.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.filtermod","page":"Reference","title":"SnoopCompile.filtermod","text":"thinned = filtermod(module, trees::AbstractVector{MethodInvalidations}; recursive=false)\n\nSelect just the cases of invalidating a method defined in module.\n\nIf recursive is false, only the roots of trees are examined (i.e., the proximal source of the invalidation must be in module). If recursive is true, then thinned contains all routes to a method in module.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.findcaller","page":"Reference","title":"SnoopCompile.findcaller","text":"methinvs = findcaller(method::Method, trees)\n\nFind a path through trees that reaches method. Returns a single MethodInvalidations object.\n\nExamples\n\nSuppose you know that loading package SomePkg triggers invalidation of f(data). You can find the specific source of invalidation as follows:\n\nf(data)                             # run once to force compilation\nm = @which f(data)\nusing SnoopCompile\ntrees = invalidation_trees(@snoopr using SomePkg)\nmethinvs = findcaller(m, trees)\n\nIf you don't know which method to look for, but know some operation that has had added latency, you can look for methods using @snoopi. For example, suppose that loading SomePkg makes the next using statement slow. You can find the source of trouble with\n\njulia> using SnoopCompile\n\njulia> trees = invalidation_trees(@snoopr using SomePkg);\n\njulia> tinf = @snoopi using SomePkg            # this second `using` will need to recompile code invalidated above\n1-element Array{Tuple{Float64,Core.MethodInstance},1}:\n (0.08518409729003906, MethodInstance for require(::Module, ::Symbol))\n\njulia> m = tinf[1][2].def\nrequire(into::Module, mod::Symbol) in Base at loading.jl:887\n\njulia> findcaller(m, trees)\ninserting ==(x, y::SomeType) in SomeOtherPkg at /path/to/code:100 invalidated:\n   backedges: 1: superseding ==(x, y) in Base at operators.jl:83 with MethodInstance for ==(::Symbol, ::Any) (16 children) more specific\n\n\n\n\n\n","category":"function"},{"location":"reference/#Analysis-of-@snoopi_deep","page":"Reference","title":"Analysis of @snoopi_deep","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"flatten\nexclusive\ninclusive\naccumulate_by_source\ncollect_for\nstaleinstances\ninference_triggers\ntrigger_tree\nsuggest\nisignorable\ncallerinstance\ncallingframe\nskiphigherorder\nInferenceTrigger\nruntime_inferencetime\nSnoopCompile.parcel\nSnoopCompile.write","category":"page"},{"location":"reference/#SnoopCompile.flatten","page":"Reference","title":"SnoopCompile.flatten","text":"flatten(tinf; tmin = 0.0, sortby=exclusive)\n\nFlatten the execution graph of InferenceTimingNodes returned from @snoopi_deep into a Vector of InferenceTiming frames, each encoding the time needed for inference of a single MethodInstance. By default, results are sorted by exclusive time (the time for inferring the MethodInstance itself, not including any inference of its callees); other options are sortedby=inclusive which includes the time needed for the callees, or nothing to obtain them in the order they were inferred (depth-first order).\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoopi_deep on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.002148974/0.002767166 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 1 direct children\n\njulia> using AbstractTrees; print_tree(tinf)\nInferenceTimingNode: 0.00242354/0.00303526 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 1 direct children\n└─ InferenceTimingNode: 0.000150891/0.000611721 on InferenceFrameInfo for SnoopCompile.FlattenDemo.packintype(::Int64) with 2 direct children\n   ├─ InferenceTimingNode: 0.000105318/0.000105318 on InferenceFrameInfo for MyType{Int64}(::Int64) with 0 direct children\n   └─ InferenceTimingNode: 9.43e-5/0.000355512 on InferenceFrameInfo for SnoopCompile.FlattenDemo.dostuff(::MyType{Int64}) with 2 direct children\n      ├─ InferenceTimingNode: 6.6458e-5/0.000124716 on InferenceFrameInfo for SnoopCompile.FlattenDemo.extract(::MyType{Int64}) with 2 direct children\n      │  ├─ InferenceTimingNode: 3.401e-5/3.401e-5 on InferenceFrameInfo for getproperty(::MyType{Int64}, ::Symbol) with 0 direct children\n      │  └─ InferenceTimingNode: 2.4248e-5/2.4248e-5 on InferenceFrameInfo for getproperty(::MyType{Int64}, x::Symbol) with 0 direct children\n      └─ InferenceTimingNode: 0.000136496/0.000136496 on InferenceFrameInfo for SnoopCompile.FlattenDemo.domath(::Int64) with 0 direct children\n\nNote the printing of getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol): it shows the specific Symbol, here :x, that getproperty was inferred with. This reflects constant-propagation in inference.\n\nThen:\n\njulia> flatten(tinf; sortby=nothing)\n8-element Vector{SnoopCompileCore.InferenceTiming}:\n InferenceTiming: 0.002423543/0.0030352639999999998 on InferenceFrameInfo for Core.Compiler.Timings.ROOT()\n InferenceTiming: 0.000150891/0.0006117210000000001 on InferenceFrameInfo for SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.000105318/0.000105318 on InferenceFrameInfo for SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 9.43e-5/0.00035551200000000005 on InferenceFrameInfo for SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 6.6458e-5/0.000124716 on InferenceFrameInfo for SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 3.401e-5/3.401e-5 on InferenceFrameInfo for getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, ::Symbol)\n InferenceTiming: 2.4248e-5/2.4248e-5 on InferenceFrameInfo for getproperty(::SnoopCompile.FlattenDemo.MyType{Int64}, x::Symbol)\n InferenceTiming: 0.000136496/0.000136496 on InferenceFrameInfo for SnoopCompile.FlattenDemo.domath(::Int64)\n\njulia> flatten(tinf; tmin=1e-4)                        # sorts by exclusive time (the time before the '/')\n4-element Vector{SnoopCompileCore.InferenceTiming}:\n InferenceTiming: 0.000105318/0.000105318 on InferenceFrameInfo for SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 0.000136496/0.000136496 on InferenceFrameInfo for SnoopCompile.FlattenDemo.domath(::Int64)\n InferenceTiming: 0.000150891/0.0006117210000000001 on InferenceFrameInfo for SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.002423543/0.0030352639999999998 on InferenceFrameInfo for Core.Compiler.Timings.ROOT()\n\njulia> flatten(tinf; sortby=inclusive, tmin=1e-4)      # sorts by inclusive time (the time after the '/')\n6-element Vector{SnoopCompileCore.InferenceTiming}:\n InferenceTiming: 0.000105318/0.000105318 on InferenceFrameInfo for SnoopCompile.FlattenDemo.MyType{Int64}(::Int64)\n InferenceTiming: 6.6458e-5/0.000124716 on InferenceFrameInfo for SnoopCompile.FlattenDemo.extract(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 0.000136496/0.000136496 on InferenceFrameInfo for SnoopCompile.FlattenDemo.domath(::Int64)\n InferenceTiming: 9.43e-5/0.00035551200000000005 on InferenceFrameInfo for SnoopCompile.FlattenDemo.dostuff(::SnoopCompile.FlattenDemo.MyType{Int64})\n InferenceTiming: 0.000150891/0.0006117210000000001 on InferenceFrameInfo for SnoopCompile.FlattenDemo.packintype(::Int64)\n InferenceTiming: 0.002423543/0.0030352639999999998 on InferenceFrameInfo for Core.Compiler.Timings.ROOT()\n\nAs you can see, sortby affects not just the order but also the selection of frames; with exclusive times, dostuff did not on its own rise above threshold, but it does when using inclusive times.\n\nSee also: accumulate_by_source.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompileCore.exclusive","page":"Reference","title":"SnoopCompileCore.exclusive","text":"exclusive(frame)\n\nReturn the time spent inferring frame, not including the time needed for any of its callees.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompileCore.inclusive","page":"Reference","title":"SnoopCompileCore.inclusive","text":"inclusive(frame)\n\nReturn the time spent inferring frame and its callees.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.accumulate_by_source","page":"Reference","title":"SnoopCompile.accumulate_by_source","text":"accumulate_by_source(flattened; tmin = 0.0, by=exclusive)\n\nAdd the inference timings for all MethodInstances of a single Method together. flattened is the output of flatten. Returns a list of (t, method) tuples.\n\nWhen the accumulated time for a Method is large, but each instance is small, it indicates that it is being inferred for many specializations (which might include specializations with different constants).\n\nExample\n\nWe'll use SnoopCompile.flatten_demo, which runs @snoopi_deep on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.flatten_demo()\nInferenceTimingNode: 0.002148974/0.002767166 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 1 direct children\n\njulia> accumulate_by_source(flatten(tinf))\n7-element Vector{Tuple{Float64, Method}}:\n (6.0012999999999996e-5, getproperty(x, f::Symbol) in Base at Base.jl:33)\n (6.7714e-5, extract(y::SnoopCompile.FlattenDemo.MyType) in SnoopCompile.FlattenDemo at /pathto/SnoopCompile/src/deep_demos.jl:35)\n (9.421e-5, dostuff(y) in SnoopCompile.FlattenDemo at /pathto/SnoopCompile/src/deep_demos.jl:44)\n (0.000112057, SnoopCompile.FlattenDemo.MyType{T}(x) where T in SnoopCompile.FlattenDemo at /pathto/SnoopCompile/src/deep_demos.jl:34)\n (0.000133895, domath(x) in SnoopCompile.FlattenDemo at /pathto/SnoopCompile/src/deep_demos.jl:40)\n (0.000154382, packintype(x) in SnoopCompile.FlattenDemo at /pathto/SnoopCompile/src/deep_demos.jl:36)\n (0.003165266, ROOT() in Core.Compiler.Timings at compiler/typeinfer.jl:75)\n\nCompared to the output from flatten, the two inferences passes on getproperty have been consolidated into a single aggregate call.\n\n\n\n\n\nmtrigs = accumulate_by_source(Method, itrigs::AbstractVector{InferenceTrigger})\n\nConsolidate inference triggers via their caller method. mtrigs is a vector of Method=>list pairs, where list is a list of InferenceTriggers.\n\n\n\n\n\nloctrigs = accumulate_by_source(itrigs::AbstractVector{InferenceTrigger})\n\nAggregate inference triggers by location (function, file, and line number) of the caller.\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_demo:\n\njulia> itrigs = inference_triggers(SnoopCompile.itrigs_demo())\n2-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:763)\n Inference triggered to call MethodInstance for double(::Float64) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:763)\n\njulia> accumulate_by_source(itrigs)\n1-element Vector{SnoopCompile.LocationTriggers}:\n    calldouble1 at /pathto/SnoopCompile/src/parcel_snoopi_deep.jl:762 (2 callees from 1 callers)\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.collect_for","page":"Reference","title":"SnoopCompile.collect_for","text":"list = collect_for(m::Method, tinf::InferenceTimingNode)\nlist = collect_for(m::MethodInstance, tinf::InferenceTimingNode)\n\nCollect all InferenceTimingNodes (descendants of tinf) that match m.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.staleinstances","page":"Reference","title":"SnoopCompile.staleinstances","text":"staleinstances(tinf::InferenceTimingNode)\n\nReturn a list of InferenceTimingNodes corresponding to MethodInstances that have \"stale\" code (specifically, CodeInstances with outdated max_world world ages). These may be a hint that invalidation occurred while running the workload provided to @snoopi_deep, and consequently an important origin of (re)inference.\n\nwarning: Warning\nstaleinstances only looks retrospectively for stale code; it does not distinguish whether the code became stale while running @snoopi_deep from whether it was already stale before execution commenced.\n\nWhile staleinstances is recommended as a useful \"sanity check\" to run before performing a detailed analysis of inference, any serious examination of invalidation should use @snoopr.\n\nFor more information about world age, see https://docs.julialang.org/en/v1/manual/methods/#Redefining-Methods.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.inference_triggers","page":"Reference","title":"SnoopCompile.inference_triggers","text":"itrigs = inference_triggers(tinf::InferenceTimingNode; exclude_toplevel=true)\n\nCollect the \"triggers\" of inference, each a fresh entry into inference via a call dispatched at runtime. All the entries in itrigs are previously uninferred, or are freshly-inferred for specific constant inputs.\n\nexclude_toplevel determines whether calls made from the REPL, include, or test suites are excluded.\n\nExample\n\nWe'll use SnoopCompile.itrigs_demo, which runs @snoopi_deep on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.itrigs_demo()\nInferenceTimingNode: 0.004490576/0.004711168 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 2 direct children\n\njulia> itrigs = inference_triggers(tinf)\n2-element Vector{InferenceTrigger}:\n Inference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/deep_demos.jl:86) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/deep_demos.jl:87)\n Inference triggered to call MethodInstance for double(::Float64) from calldouble1 (/pathto/SnoopCompile/src/deep_demos.jl:86) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/deep_demos.jl:87)\n\njulia> edit(itrigs[1])     # opens an editor at the spot in the caller\n\njulia> ascend(itrigs[2])   # use Cthulhu to inspect the stacktrace (caller is the second item in the trace)\nChoose a call for analysis (q to quit):\n >   double(::Float64)\n       calldouble1 at /pathto/SnoopCompile/src/deep_demos.jl:86 => calldouble2(::Vector{Vector{Any}}) at /pathto/SnoopCompile/src/deep_demos.jl:87\n         calleach(::Vector{Vector{Vector{Any}}}) at /pathto/SnoopCompile/src/deep_demos.jl:88\n...\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.suggest","page":"Reference","title":"SnoopCompile.suggest","text":"suggest(itrig::InferenceTrigger)\n\nAnalyze itrig and attempt to suggest an interpretation or remedy. This returns a structure of type Suggested; the easiest thing to do with the result is to show it; however, you can also filter a list of suggestions.\n\nExample\n\njulia> itrigs = inference_triggers(tinf);\n\njulia> sugs = suggest.(itrigs);\n\njulia> sugs_important = filter(!isignorable, sugs)    # discard the ones that probably don't need to be addressed\n\nwarning: Warning\nSuggestions are approximate at best; most often, the proposed fixes should not be taken literally, but instead taken as a hint about the \"outcome\" of a particular runtime dispatch incident. The suggestions target calls made with non-inferrable argumets, but often the best place to fix the problem is at an earlier stage in the code, where the argument was first computed.You can get much deeper insight via ascend (and Cthulhu generally), and even stacktrace is often useful. Suggestions are intended to be a quick and easier-to-comprehend first pass at analyzing an inference trigger.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.isignorable","page":"Reference","title":"SnoopCompile.isignorable","text":"isignorable(s::Suggested)\n\nReturns true if s is unlikely to be an inference problem in need of fixing.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.callerinstance","page":"Reference","title":"SnoopCompile.callerinstance","text":"mi = callerinstance(itrig::InferenceTrigger)\n\nReturn the MethodInstance mi of the caller in the selected stackframe in itrig.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.callingframe","page":"Reference","title":"SnoopCompile.callingframe","text":"itrigcaller = callingframe(itrig::InferenceTrigger)\n\n\"Step out\" one layer of the stacktrace, referencing the caller of the current frame of itrig.\n\nYou can retrieve the proximal trigger of inference with InferenceTrigger(itrigcaller).\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_demo:\n\njulia> itrig = inference_triggers(SnoopCompile.itrigs_demo())[1]\nInference triggered to call MethodInstance for double(::UInt8) from calldouble1 (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:762) inlined into MethodInstance for calldouble2(::Vector{Vector{Any}}) (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:763)\n\njulia> itrigcaller = callingframe(itrig)\nInference triggered to call MethodInstance for double(::UInt8) from calleach (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:764) with specialization MethodInstance for calleach(::Vector{Vector{Vector{Any}}})\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.skiphigherorder","page":"Reference","title":"SnoopCompile.skiphigherorder","text":"itrignew = skiphigherorder(itrig; exact::Bool=false)\n\nAttempt to skip over frames of higher-order functions that take the callee as a function-argument. This can be useful if you're analyzing inference triggers for an entire package and would prefer to assign triggers to package-code rather than Base functions like map!, broadcast, etc.\n\nExample\n\nWe collect data using the SnoopCompile.itrigs_higherorder_demo:\n\njulia> itrig = inference_triggers(SnoopCompile.itrigs_higherorder_demo())[1]\nInference triggered to call MethodInstance for double(::Float64) from mymap! (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:706) with specialization MethodInstance for mymap!(::typeof(SnoopCompile.ItrigHigherOrderDemo.double), ::Vector{Any}, ::Vector{Any})\n\njulia> callingframe(itrig)      # step out one (non-inlined) frame\nInference triggered to call MethodInstance for double(::Float64) from mymap (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:710) with specialization MethodInstance for mymap(::typeof(SnoopCompile.ItrigHigherOrderDemo.double), ::Vector{Any})\n\njulia> skiphigherorder(itrig)   # step out to frame that doesn't have `double` as a function-argument\nInference triggered to call MethodInstance for double(::Float64) from callmymap (/pathto/SnoopCompile/src/parcel_snoopi_deep.jl:711) with specialization MethodInstance for callmymap(::Vector{Any})\n\nwarn: Warn\nBy default skiphigherorder is conservative, and insists on being sure that it's the callee being passed to the higher-order function. Higher-order functions that do not get specialized (e.g., with ::Function argument types) will not be skipped over. You can pass exact=false to allow ::Function to also be passed over, but keep in mind that this may falsely skip some frames.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.InferenceTrigger","page":"Reference","title":"SnoopCompile.InferenceTrigger","text":"InferenceTrigger(callee::MethodInstance, callerframes::Vector{StackFrame}, btidx::Int, bt)\n\nOrganize information about the \"triggers\" of inference. callee is the MethodInstance requiring inference, callerframes, btidx and bt contain information about the caller. callerframes are the frame(s) of call site that triggered inference; it's a Vector{StackFrame}, rather than a single StackFrame, due to the possibility that the caller was inlined into something else, in which case the first entry is the direct caller and the last entry corresponds to the MethodInstance into which it was ultimately inlined. btidx is the index in bt, the backtrace collected upon entry into inference, corresponding to callerframes.\n\nInferenceTriggers are created by calling inference_triggers. See also: callerinstance and callingframe.\n\n\n\n\n\n","category":"type"},{"location":"reference/#SnoopCompile.runtime_inferencetime","page":"Reference","title":"SnoopCompile.runtime_inferencetime","text":"ridata = runtime_inferencetime(tinf::InferenceTimingNode; consts=true, by=inclusive)\nridata = runtime_inferencetime(tinf::InferenceTimingNode, profiledata; lidict, consts=true, by=inclusive)\n\nCompare runtime and inference-time on a per-method basis. ridata[m::Method] returns (trun, tinfer, nspecializations), measuring the approximate amount of time spent running m, inferring m, and the number of type-specializations, respectively. trun is estimated from profiling data, which the user is responsible for capturing before the call. Typically tinf is collected via @snoopi_deep on the first call (in a fresh session) to a workload, and the profiling data collected on a subsequent call. In some cases you may need to repeat the workload several times to collect enough profiling samples.\n\nprofiledata and lidict are obtained from Profile.retrieve().\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.parcel","page":"Reference","title":"SnoopCompile.parcel","text":"pc = parcel(calls; subst=[], exclusions=[]) assigns each compile statement to the module that owns the function. Perform string substitution via subst=[\"Module1\"=>\"Module2\"], and omit functions in particular modules with exclusions=[\"Module3\"]. On output, pc[:Module2] contains all the precompiles assigned to Module2.\n\nUse SnoopCompile.write(prefix, pc) to generate a series of files in directory prefix, one file per module.\n\n\n\n\n\nttot, pcs = SnoopCompile.parcel(tinf::InferenceTimingNode)\n\nParcel the \"root-most\" precompilable MethodInstances into separate modules. These can be used to generate precompile directives to cache the results of type-inference, reducing latency on first use.\n\nLoosely speaking, and MethodInstance is precompilable if the module that owns the method also has access to all the types it need to precompile the instance. When the root node of an entrance to inference is not itself precompilable, parcel examines the children (and possibly, children's children...) until it finds the first node on each branch that is precompilable. MethodInstances are then assigned to the module that owns the method.\n\nttot is the total inference time; pcs is a list of module => (tmod, pclist) pairs. For each module, tmod is the amount of inference time affiliated with methods owned by that module; pclist is a list of (t, mi) time/MethodInstance tuples.\n\nSee also: SnoopCompile.write.\n\nExample\n\nWe'll use SnoopCompile.itrigs_demo, which runs @snoopi_deep on a workload designed to yield reproducible results:\n\njulia> tinf = SnoopCompile.itrigs_demo()\nInferenceTimingNode: 0.004490576/0.004711168 on InferenceFrameInfo for Core.Compiler.Timings.ROOT() with 2 direct children\n\njulia> ttot, pcs = SnoopCompile.parcel(tinf);\n\njulia> ttot\n0.000220592\n\njulia> pcs\n1-element Vector{Pair{Module, Tuple{Float64, Vector{Tuple{Float64, Core.MethodInstance}}}}}:\n SnoopCompile.ItrigDemo => (0.000220592, [(9.8986e-5, MethodInstance for double(::Float64)), (0.000121606, MethodInstance for double(::UInt8))])\n\nSince there was only one module, ttot is the same as tmod. The ItrigDemo module had two precomilable MethodInstances, each listed with its corresponding inclusive time.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.write","page":"Reference","title":"SnoopCompile.write","text":"write(prefix::AbstractString, pc::Dict; always::Bool = false)\n\nWrite each modules' precompiles to a separate file.  If always is true, the generated function will always run the precompile statements when called, otherwise the statements will only be called during package precompilation.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Other-utilities","page":"Reference","title":"Other utilities","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"SnoopCompile.read\nSnoopCompile.format_userimg","category":"page"},{"location":"reference/#SnoopCompile.read","page":"Reference","title":"SnoopCompile.read","text":"SnoopCompile.read(\"compiledata.csv\") reads the log file produced by the compiler and returns the functions as a pair of arrays. The first array is the amount of time required to compile each function, the second is the corresponding function + types. The functions are sorted in order of increasing compilation time. (The time does not include the cost of nested compiles.)\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.format_userimg","page":"Reference","title":"SnoopCompile.format_userimg","text":"pc = format_userimg(calls; subst=[], exclusions=[]) generates precompile directives intended for your base/userimg.jl script. Use SnoopCompile.write(filename, pc) to create a file that you can include into userimg.jl.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Demos","page":"Reference","title":"Demos","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"SnoopCompile.flatten_demo\nSnoopCompile.itrigs_demo\nSnoopCompile.itrigs_higherorder_demo","category":"page"},{"location":"reference/#SnoopCompile.flatten_demo","page":"Reference","title":"SnoopCompile.flatten_demo","text":"tinf = SnoopCompile.flatten_demo()\n\nA simple demonstration of @snoopi_deep. This demo defines a module\n\nmodule FlattenDemo\n    struct MyType{T} x::T end\n    extract(y::MyType) = y.x\n    function packintype(x)\n        y = MyType{Int}(x)\n        return dostuff(y)\n    end\n    function domath(x)\n        y = x + x\n        return y*x + 2*x + 5\n    end\n    dostuff(y) = domath(extract(y))\nend\n\nIt then \"warms up\" (forces inference on) all of Julia's Base methods needed for domath, to ensure that these MethodInstances do not need to be inferred when we collect the data. It then returns the results of\n\n@snoopi_deep FlattenDemo.packintypes(1)\n\nSee flatten for an example usage.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.itrigs_demo","page":"Reference","title":"SnoopCompile.itrigs_demo","text":"tinf = SnoopCompile.itrigs_demo()\n\nA simple demonstration of collecting inference triggers. This demo defines a module\n\nmodule ItrigDemo\n@noinline double(x) = 2x\n@inline calldouble1(c) = double(c[1])\ncalldouble2(cc) = calldouble1(cc[1])\ncalleach(ccs) = (calldouble2(ccs[1]), calldouble2(ccs[2]))\nend\n\nIt then \"warms up\" (forces inference on) calldouble2(::Vector{Vector{Any}}), calldouble1(::Vector{Any}), double(::Int):\n\ncc = [Any[1]]\nItrigDemo.calleach([cc,cc])\n\nThen it collects and returns inference data using\n\ncc1, cc2 = [Any[0x01]], [Any[1.0]]\n@snoopi_deep ItrigDemo.calleach([cc1, cc2])\n\nThis does not require any new inference for calldouble2 or calldouble1, but it does force inference on double with two new types. See inference_triggers to see what gets collected and returned.\n\n\n\n\n\n","category":"function"},{"location":"reference/#SnoopCompile.itrigs_higherorder_demo","page":"Reference","title":"SnoopCompile.itrigs_higherorder_demo","text":"tinf = SnoopCompile.itrigs_higherorder_demo()\n\nA simple demonstration of handling higher-order methods with inference triggers. This demo defines a module\n\nmodule ItrigHigherOrderDemo\ndouble(x) = 2x\n@noinline function mymap!(f, dst, src)\n    for i in eachindex(dst, src)\n        dst[i] = f(src[i])\n    end\n    return dst\nend\n@noinline mymap(f::F, src) where F = mymap!(f, Vector{Any}(undef, length(src)), src)\ncallmymap(src) = mymap(double, src)\nend\n\nThe key feature of this set of definitions is that the function double gets passed as an argument through mymap and mymap! (the latter are higher-order functions).\n\nIt then \"warms up\" (forces inference on) callmymap(::Vector{Any}), mymap(::typeof(double), ::Vector{Any}), mymap!(::typeof(double), ::Vector{Any}, ::Vector{Any}) and double(::Int):\n\nItrigHigherOrderDemo.callmymap(Any[1, 2])\n\nThen it collects and returns inference data using\n\n@snoopi_deep ItrigHigherOrderDemo.callmymap(Any[1.0, 2.0])\n\nwhich forces inference for double(::Float64).\n\nSee skiphigherorder for an example using this demo.\n\n\n\n\n\n","category":"function"},{"location":"tutorial/#tutorial","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"Certain concepts and types will appear repeatedly, so it's worth spending a little time to familiarize yourself at the outset. You can find a more expansive version of this page in this blog post.","category":"page"},{"location":"tutorial/#MethodInstances,-type-inference,-and-backedges","page":"Tutorial on the foundations","title":"MethodInstances, type-inference, and backedges","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"Our first goal is to understand how code connects together. We'll try some experiments using the following:","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"double(x::Real) = 2x\ncalldouble(container) = double(container[1])\ncalldouble2(container) = calldouble(container)","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"DocTestSetup = quote\n   double(x::Real) = 2x\n   calldouble(container) = double(container[1])\n   calldouble2(container) = calldouble(container)\nend","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"Let's create a container and run this code:","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"julia> c64 = [1.0]\n1-element Vector{Float64}:\n 1.0\n\njulia> calldouble2(c64)\n2.0","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"Using the MethodAnalysis package, we can get some insights into how Julia represents this code and its compilation dependencies:","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"julia> using MethodAnalysis\n\njulia> mi = methodinstance(double, (Float64,))\nMethodInstance for double(::Float64)\n\njulia> using AbstractTrees\n\njulia> print_tree(mi)\nMethodInstance for double(::Float64)\n└─ MethodInstance for calldouble(::Vector{Float64})\n   └─ MethodInstance for calldouble2(::Vector{Float64})","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"This indicates that the result for type-inference on calldouble2(::Vector{Float64}) depended on the result for calldouble(::Vector{Float64}), which in turn depended on double(::Float64).","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"Now let's create a new container, one with abstract element type, so that Julia's type-inference cannot accurately predict the type of elements in the container:","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"julia> cabs = AbstractFloat[1.0f0]      # put a Float32 in a Vector{AbstractFloat}\n1-element Vector{AbstractFloat}:\n 1.0f0\n\njulia> calldouble2(cabs)\n2.0f0","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"Now let's look at the available instances:","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"julia> mis = methodinstances(double)\n3-element Vector{Core.MethodInstance}:\n MethodInstance for double(::Float64)\n MethodInstance for double(::AbstractFloat)\n MethodInstance for double(::Float32)\n\njulia> print_tree(mis[1])\nMethodInstance for double(::Float64)\n└─ MethodInstance for calldouble(::Vector{Float64})\n   └─ MethodInstance for calldouble2(::Vector{Float64})\n\njulia> print_tree(mis[2])\nMethodInstance for double(::AbstractFloat)\n\njulia> print_tree(mis[3])\nMethodInstance for double(::Float32)","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"double(::Float64) has backedges to calldouble and calldouble2, but the second two do not because double was only called via runtime dispatch. However, calldouble has backedges to calldouble2","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"julia> mis = methodinstances(calldouble)\n2-element Vector{Core.MethodInstance}:\n MethodInstance for calldouble(::Vector{Float64})\n MethodInstance for calldouble(::Vector{AbstractFloat})\n\njulia> print_tree(mis[1])\nMethodInstance for calldouble(::Vector{Float64})\n└─ MethodInstance for calldouble2(::Vector{Float64})\n\njulia> print_tree(mis[2])\nMethodInstance for calldouble(::Vector{AbstractFloat})\n└─ MethodInstance for calldouble2(::Vector{AbstractFloat})","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"because Vector{AbstractFloat} is a concrete type, whereas AbstractFloat is not.","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"If we create c32 = [1.0f0] and then calldouble2(c32), we would also see backedges from double(::Float32) all the way back to calldouble2(::Vector{Float32}).","category":"page"},{"location":"tutorial/#Precompilation","page":"Tutorial on the foundations","title":"Precompilation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"During package precompilation, Julia creates a *.ji file typically stored in .julia/compiled/v1.x/, where 1.x is your version of Julia. Your *.ji file might just have definitions of constants, types, and methods, but optionally you can also include the results of type-inference. This happens automatically if you run code while your package is being built, but generally the recommended procedure is to add precompile directives.","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"Let's turn the example above into a package. In a fresh session,","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"(@v1.6) pkg> generate SnoopCompileDemo\n  Generating  project SnoopCompileDemo:\n    SnoopCompileDemo/Project.toml\n    SnoopCompileDemo/src/SnoopCompileDemo.jl\n\njulia> open(\"SnoopCompileDemo/src/SnoopCompileDemo.jl\", \"w\") do io\n           write(io, \"\"\"\n           module SnoopCompileDemo\n\n           double(x::Real) = 2x\n           calldouble(container) = double(container[1])\n           calldouble2(container) = calldouble(container)\n\n           precompile(calldouble2, (Vector{Float32},))\n           precompile(calldouble2, (Vector{Float64},))\n           precompile(calldouble2, (Vector{AbstractFloat},))\n\n           end\n           \"\"\")\n       end\n282\n\njulia> push!(LOAD_PATH, \"SnoopCompileDemo/\")\n4-element Vector{String}:\n \"@\"\n \"@v#.#\"\n \"@stdlib\"\n \"SnoopCompileDemo/\"\n\njulia> using SnoopCompileDemo\n[ Info: Precompiling SnoopCompileDemo [44c70eed-03a3-46c0-8383-afc033fb6a27]\n\njulia> using MethodAnalysis\n\njulia> methodinstances(SnoopCompileDemo.double)\n3-element Vector{Core.MethodInstance}:\n MethodInstance for double(::Float32)\n MethodInstance for double(::Float64)\n MethodInstance for double(::AbstractFloat)","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"Because of those precompile statements, the MethodInstances exist after loading the package even though we haven't run the code in this session–not because it precompiled them when the package loaded, but because they were precompiled during the Precompiling SnoopCompileDemo... phase, stored to *.ji file, and then reloaded whenever we use the package. You can also verify that the same backedges get created as when we ran this code interactively above.","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"By having these MethodInstances \"pre-loaded\" we can save some of the time needed to run type-inference: not much time in this case because the code is so simple, but for more complex methods the savings can be substantial.","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"This code got cached in SnoopCompileDemo.ji. It's worth noting that even though the precompile directive got issued from this package, it might save MethodInstances for methods defined in other packages. For example, Julia does not come pre-built with the inferred code for Int * Float32: in a fresh session,","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"julia> using MethodAnalysis\n\njulia> mi = methodinstance(*, (Int, Float32))\n","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"returns nothing (the MethodInstance doesn't exist), whereas if we've loaded SnoopCompileDemo then","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"julia> mi = methodinstance(*, (Int, Float32))\nMethodInstance for *(::Int64, ::Float32)\n\njulia> mi.def\n*(x::Number, y::Number) in Base at promotion.jl:322","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"So even though the method is defined in Base, because SnoopCompileDemo needed this code it got stashed in SnoopCompileDemo.ji.","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"The ability to cache MethodInstances from code defined in other packages or libraries is fundamental to latency reduction; however, it has significant limitations.  Most crucially, *.ji files can only hold code they \"own,\" either:","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"to a method defined in the package\nthrough a chain of backedges to methods owned by the package","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"If we add","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"precompile(*, (Int, Float16))","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"to the definition of SnoopCompileDemo.jl, nothing happens:","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"julia> mi = methodinstance(*, (Int, Float16))\n                                                 # nothing","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"because there is no \"chain of ownership\" to SnoopCompileDemo. Consequently, we can't precompile methods defined in other modules in and of themselves; we can only do it if those methods are linked by backedges to this package.","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"Because backedges are created during successful type-inference, the consequence is that precompilation works better when type inference succeeds. For some packages, time invested in improving inferrability can make your precompile directives work better.","category":"page"},{"location":"tutorial/","page":"Tutorial on the foundations","title":"Tutorial on the foundations","text":"DocTestSetup = nothing","category":"page"},{"location":"#SnoopCompile.jl","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"SnoopCompile \"snoops\" on the Julia compiler, causing it to record the functions and argument types it's compiling.  From these lists of methods, you can generate lists of precompile directives that may reduce the latency between loading packages and using them to do \"real work.\"","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"SnoopCompile can also detect and analyze method cache invalidations, which occur when new method definitions alter dispatch in a way that forces Julia to discard previously-compiled code. Any later usage of invalidated methods requires recompilation. Invalidation can trigger a domino effect, in which all users of invalidated code also become invalidated, propagating all the way back to the top-level call. When a source of invalidation can be identified and either eliminated or mitigated, you can reduce the amount of work that the compiler needs to repeat and take better advantage of precompilation.","category":"page"},{"location":"#Background","page":"SnoopCompile.jl","title":"Background","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Julia uses Just-in-time (JIT) compilation to generate the code that runs on your CPU. Broadly speaking, there are two major steps: inference and code generation. Inference is the process of determining the type of each object, which in turn determines which specific methods get called; once type inference is complete, code generation performs optimizations and ultimately generates the assembly language (native code) used on CPUs. Some aspects of this process are documented here.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Every time you load a package in a fresh Julia session, the methods you use need to be JIT-compiled, and this contributes to the latency of using the package. In some circumstances, you can partially cache the results of compilation to a file (the *.ji files that live in your ~/.julia/compiled directory) to reduce the burden when your package is used. This is called precompilation. Unfortunately, precompilation is not as comprehensive as one might hope. Currently, Julia is only able to save inference results (not native code) in the *.ji files, and thus precompilation only eliminates the time needed for type inference. Moreover, there are some significant constraints that sometimes prevent Julia from saving even the inference results–for example, currently you cannot cache inference results for \"top level\" calls to methods defined in Julia or other packages, even if you are calling them with types defined in your package. Finally, what does get saved can sometimes be invalidated by loading other packages.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Despite these limitations, there are many cases where precompilation can substantially reduce latency. SnoopCompile is designed to try to allow you to analyze the costs of JIT-compilation, identify key bottlenecks that contribute to latency, and set up precompile directives to see whether it produces measurable benefits.","category":"page"},{"location":"#Who-should-use-this-package","page":"SnoopCompile.jl","title":"Who should use this package","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"SnoopCompile is intended primarily for package developers who want to improve the experience for their users. Because the results of SnoopCompile are typically stored in the *.ji precompile files, users automatically get the benefit of any latency reductions achieved by adding precompile directives to the source code of your package.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"PackageCompiler is an alternative that non-developer users may want to consider for their own workflow. It builds an entire system image (Julia + a set of selected packages) and caches both the results of type inference and the native code. Typically, PackageCompiler reduces latency more than just \"plain\" precompile directives. However, PackageCompiler does have significant downsides, of which the largest is that it is incompatible with package updates–any packages built into your system image cannot be updated without rebuilding the entire system. Particularly for people who develop or frequently update their packages, the downsides of PackageCompiler may outweigh its benefits.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Finally, another alternative for reducing latency without any modifications to package files is Revise. It can be used in conjunction with SnoopCompile.","category":"page"},{"location":"#workflow","page":"SnoopCompile.jl","title":"A note on Julia versions and the recommended workflow","text":"","category":"section"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"SnoopCompile is closely intertwined with Julia's own internals. Some \"data collection\" and analysis features are available only on newer versions of Julia. In particular, some of the most powerful tools were made possible through several additions made in Julia 1.6; SnoopCompile just exposes these tools in convenient form.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"If you're a developer looking to reduce the latency of your packages, you are strongly encouraged to use SnoopCompile on Julia 1.6 or later. The fruits of your labors will often reduce latency even for users of earlier Julia versions, but your ability to understand what changes need to be made will be considerably enhanced by using the latest tools.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"For developers who can use Julia 1.6+, the recommended sequence is:","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Check for invalidations, and if egregious make fixes before proceeding further\nRecord inference data with @snoopi_deep. Analyze the data to:\nadjust method specialization in your package or its dependencies\nfix problems in type inference\nadd precompile directives","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Under 2, the first two sub-points can often be done at the same time; the last item is best done as a final step, because the specific precompile directives needed depend on the state of your code, and a few fixes in specialization and/or type inference can alter or even decrease the number of necessary precompile directives.","category":"page"},{"location":"","page":"SnoopCompile.jl","title":"SnoopCompile.jl","text":"Although there are other tools within SnoopCompile available, most developers can probably stop after the steps above. The documentation will describe the tools in this order, followed by descriptions of additional and older tools.","category":"page"}]
}
